{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Stylegan2-ada.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rEHmTroCgxSn",
        "QdaHV4W2dXOF"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cac38f8d43a84ec29dd2076ad7d8da09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cb8a60b3a51f4883adca5135e8ead948",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1b6f2e85b1334a67949f9fba03c7fb3d",
              "IPY_MODEL_edb08bda75a243deae18989b518ac6c3"
            ]
          }
        },
        "cb8a60b3a51f4883adca5135e8ead948": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b6f2e85b1334a67949f9fba03c7fb3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f3d7f751f9d448d0bbaad17183b99525",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Generating images: 300",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6f036586ea04c0e97b36337f4dfd23c"
          }
        },
        "edb08bda75a243deae18989b518ac6c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_03f87079d67a42698d4e21c0a140e1dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8750e9f2bce1419298b83db8e02f1919"
          }
        },
        "f3d7f751f9d448d0bbaad17183b99525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6f036586ea04c0e97b36337f4dfd23c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03f87079d67a42698d4e21c0a140e1dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8750e9f2bce1419298b83db8e02f1919": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56fc021f48ad4b2d9b0ad37a55b557b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b79191a9c7a24791b94d84b53562fcd9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ee291997d8f94a7bad1c5273c0c94b62",
              "IPY_MODEL_d73267fc01ef4b5eac3ae430d821ed06"
            ]
          }
        },
        "b79191a9c7a24791b94d84b53562fcd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee291997d8f94a7bad1c5273c0c94b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cb4975a2be1949118de11027212d4d69",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Saving images: 300",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4458838a06e6460ba04c95764d4f4dd6"
          }
        },
        "d73267fc01ef4b5eac3ae430d821ed06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d3139d7b7ef74f80b463ecd1606bc9ac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 300,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 300,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a8683d536cd4a06b43f72f842623708"
          }
        },
        "cb4975a2be1949118de11027212d4d69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4458838a06e6460ba04c95764d4f4dd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3139d7b7ef74f80b463ecd1606bc9ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a8683d536cd4a06b43f72f842623708": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saadz-khan/StyleGAN-client/blob/master/StyleGAN-ada%20interpolation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxvANTYPYASV"
      },
      "source": [
        "#Training StyleGAN2 on Colab\n",
        "\n",
        "Use Colab Pro+ ($49.99/month), Colab Pro ($10.00/month) to get a couple extra hours of training time in per session and do not run into low-RAM issues.\n",
        "\n",
        "Recommended: Colab Pro ($10.00 per month)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95xTpqpzYBmy"
      },
      "source": [
        "## Setting up only once. Kindly check the option\n",
        "<center>  \n",
        "\n",
        "![image](https://media.giphy.com/media/IFRhbMJSy3IaNZulCk/source.gif?cid=790b76116b562dd18158c804953ae06343645e756f8ed52d&rid=source.gif&ct=g)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPI5E5y0pujD"
      },
      "source": [
        "## Custom Training StyleGan2-ADA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI_i1MwgpzOD"
      },
      "source": [
        "StyleGAN2-ADA only work with Tensorflow 1. Run the next cell before anything else to make sure we’re using TF1 and not TF2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKYAU7Wub3WW",
        "outputId": "eaeccc1d-7cc8-4bef-d6a2-3bb93c93e127"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51ei6d5kxVDm",
        "outputId": "ac31664f-412d-4e25-85fb-b3373165deee"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 21 07:07:27 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YcUMPQp6ipP"
      },
      "source": [
        "# Install Repo to Google Drive\n",
        "\n",
        "Colab is a little funky with training. I’ve found the best way to do this is to install the repo directly into your Google Drive folder.\n",
        "\n",
        "First, mount your Drive to the Colab notebook: |\n",
        "\n",
        "## Mounting Google Drive\n",
        "So I’m actually gonna install my entire repo directly into your Google Drive. This will make the setup a little easier.\n",
        "\n",
        "First, connect your Drive to Colab. Gif explains how to:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.giphy.com/media/WKMU25D95cjqWw9GCS/source.gif?cid=790b7611b611ba7d6a09693279bea7a3e08cb1e976b1b827&rid=source.gif&ct=g\" alt=\"animated-gif2\" />\n",
        "</p>  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxxYlEKI9Gis",
        "outputId": "fafca3bd-1af4-45b6-d01a-a391b7df437c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epV6TDzAjox1"
      },
      "source": [
        "## Installing and Updating the Repo\n",
        "Next, run this cell. If you’re already installed the repo,  \n",
        "It will skip the installation process and change into the repo’s directory. If you haven’t installed it, it will install all the files necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HX77jscX2zV",
        "outputId": "d56834da-ba56-431a-a721-dcd044083c46"
      },
      "source": [
        "import os\n",
        "if os.path.isdir(\"/content/drive/My Drive/colab-sg2-ada\"):\n",
        "    %cd \"/content/drive/My Drive/colab-sg2-ada/stylegan2-ada\"\n",
        "else:\n",
        "    #install script\n",
        "    %cd \"/content/drive/My Drive/\"\n",
        "    !mkdir colab-sg2-ada\n",
        "    %cd colab-sg2-ada\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada\n",
        "    %cd stylegan2-ada\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/colab-sg2-ada/stylegan2-ada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7oj_kBaemol",
        "outputId": "5da03129-45c5-4f4b-93c2-203e5b66351f"
      },
      "source": [
        "%cd \"/content/drive/My Drive/colab-sg2-ada/stylegan2-ada\"\n",
        "!git config --global user.name \"test\"\n",
        "!git config --global user.email \"test@test.com\"\n",
        "!git fetch origin\n",
        "!git checkout origin/main -- train.py"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/colab-sg2-ada/stylegan2-ada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CyylK3CeWmR"
      },
      "source": [
        "# Dataset Requirements\n",
        "\n",
        "The following are the dataset requirements\n",
        "\n",
        "- All the images in the dataset folder must be RGB (bitdepth = 24) or grayscale (bitdepth=8) \n",
        "- All the images must have the same resolution and squared.\n",
        "**Resolution Requirements:**\n",
        "- Images must be squared for this repository (For easy bulk resizing I would recommend this website [Click Link](https://bulkresizephotos.com/) \n",
        "You can add photos in bulk and it is super easy to resize it. It takes time though about 30 minutes for about a 1000 images resizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEHmTroCgxSn"
      },
      "source": [
        "## Some Custom Dataset \n",
        "If you would like to experiment. These are all art based you can check it out and Enjoy working with it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKnegsXPgvLR",
        "outputId": "75996fd5-4eb9-4110-df12-c7ce2533e40f"
      },
      "source": [
        "#@title Choose Your Dataset\n",
        "\n",
        "Dataset = 'Ukiyo_e' #@param [\"Abstract_Expressionism\", \"Art_Nouveau_Modern\", \"Baroque\", \"Color_Field_Painting\", \"Cubism\", \"Expressionism\", \"Fauvism\", \"High_Renaissance\", \"Impressionism\", \"Mannerism_Late_Renaissance\", \"Minimalism\", \"Naive_Art_Primitivism\", \"Northern_Renaissance\", \"pop_art\", \"Post_Impressionism\", \"Realism\", \"Rococo\", \"Romanticism\", \"Symbolism\", \"Ukiyo_e\"]\n",
        "%cd \"/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/datasets\"\n",
        "\n",
        "if Dataset is 'Minimalism':\n",
        "  !wget https://archive.org/download/StyleGANDatasets/Minimalism.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Minimalism.zip' \n",
        "  dataset_path= \"./Minimalism\"\n",
        "\n",
        "if Dataset is \"Cubism\":\n",
        "  !wget https://archive.org/download/StyleGANDatasets/Cubism.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Cubism.zip' \n",
        "  dataset_path= \"./Cubism\" \n",
        "\n",
        "if Dataset is \"Abstract_Expressionism\":\n",
        "  !wget https://archive.org/download/StyleGANDatasets/Abstract_Expressionism.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Abstract_Expressionism.zip' \n",
        "  dataset_path= \"./Abstract_Expressionism\"\n",
        "\n",
        "if Dataset is \"Color_Field_Painting\":\n",
        "  !wget https://archive.org/download/StyleGANDatasets/Color_Field_Painting.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Color_Field_Painting.zip' \n",
        "  dataset_path= \"./Color_Field_Painting\"\n",
        " \n",
        "if Dataset is \"Fauvism\":\n",
        "  !wget https://archive.org/download/StyleGANDatasets/Fauvism.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Fauvism.zip' \n",
        "  dataset_path= \"./Fauvism\"\n",
        "\n",
        "if Dataset is \"High_Renaissance\":\n",
        "  !wget https://archive.org/download/StyleGANDatasets/High_Renaissance.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/High_Renaissance.zip' \n",
        "  dataset_path= \"./High_Renaissance\"\n",
        "\n",
        "if Dataset is \"Mannerism_Late_Renaissance\": \n",
        "  !wget https://archive.org/download/StyleGANDatasets/Mannerism_Late_Renaissance.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Mannerism_Late_Renaissance.zip' \n",
        "  dataset_path= \"./Mannerism_Late_Renaissance\"\n",
        "\n",
        "if Dataset is \"Rococo\":\n",
        "  !wget https://archive.org/download/StyleGANDatasets/Rococo.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Rococo.zip' \n",
        "  dataset_path= \"./Rococo\" \n",
        "\n",
        "if Dataset is \"Ukiyo_e\":\n",
        "  !wget https://archive.org/download/StyleGANDatasets/Ukiyo_e.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Ukiyo_e.zip' \n",
        "  dataset_path= \"./Ukiyo_e\"\n",
        "\n",
        "if Dataset is \"pop_art\":\n",
        "  !wget https://archive.org/download/StyleGANDatasets/pop_art.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/pop_art.zip' \n",
        "  dataset_path= \"./pop_art\"\n",
        "\n",
        "if Dataset is \"Art_Nouveau_Modern\":\n",
        "  !wget https://archive.org/download/Art_Nouveau_Modern/Art_Nouveau_Modern.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Art_Nouveau_Modern.zip' \n",
        "  dataset_path= \"./Art_Nouveau_Modern\"\n",
        "\n",
        "if Dataset is \"baroque\":\n",
        "  !wget https://archive.org/download/BaroqueDataset/baroque.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/baroque.zip' \n",
        "  dataset_path= \"./baroque\"\n",
        "\n",
        "if Dataset is \"Expressionism\":\n",
        "  !wget https://archive.org/download/expressionism/expressionism.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/expressionism.zip' \n",
        "  dataset_path= \"./expressionism\"\n",
        "\n",
        "if Dataset is \"Naive_Art_Primitivism\":\n",
        "  !wget https://archive.org/download/Naive_Art_Primitivism/Naive_Art_Primitivism.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Naive_Art_Primitivism.zip' \n",
        "  dataset_path= \"./Naive_Art_Primitivism\"\n",
        "\n",
        "if Dataset is \"Impressionism\":\n",
        "  !wget https://archive.org/download/ImpressionismArt/Impressionism.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Impressionism.zip' \n",
        "  dataset_path= \"./Impressionism\" \n",
        "\n",
        "if Dataset is \"Northern_Renaissance\":\n",
        "  !wget https://archive.org/download/Northern_Renaissance/Northern_Renaissance.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Northern_Renaissance.zip' \n",
        "  dataset_path= \"./Northern_Renaissance\"\n",
        "\n",
        "if Dataset is \"Post_Impressionism\":\n",
        "  !wget https://archive.org/download/Post_Impressionism/Post_Impressionism.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Post_Impressionism.zip' \n",
        "  dataset_path= \"./Post_Impressionism\"\n",
        "\n",
        "if Dataset is \"Realism\":\n",
        "  !wget https://archive.org/download/RealismData/Realism.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Realism.zip' \n",
        "  dataset_path= \"./Realism\"  \n",
        "\n",
        "if Dataset is \"Romanticism\":\n",
        "  !wget https://archive.org/download/RomanticismData/Romanticism.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Romanticism.zip'    \n",
        "  dataset_path= \"./Romanticism\" \n",
        "\n",
        "if Dataset is \"Symbolism\":\n",
        "  !wget https://archive.org/download/SymbolismData/Symbolism.zip -q --show-progress\n",
        "  !unzip '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Symbolism.zip'    \n",
        "  dataset_path= \"./Symbolism\"\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/datasets\n",
            "Ukiyo_e.zip           0%[                    ]   7.49M   398KB/s    eta 92m 41s^C\n",
            "unzip:  cannot find or open /content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Ukiyo_e.zip, /content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Ukiyo_e.zip.zip or /content/drive/MyDrive/colab-sg2-ada/stylegan2-ada-pytorch/datasets/Ukiyo_e.zip.ZIP.\n",
            "/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdaHV4W2dXOF"
      },
      "source": [
        "## Adding the dataset (Do not run) For testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU-9VfuCgBw1"
      },
      "source": [
        "!pip install kaggle-cli\n",
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-5CC_EvddrW",
        "outputId": "4c988288-c28c-4e05-a545-43afbade8eb6"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/\n",
        "!mkdir ./data\n",
        "!mkdir ./data/robgon-abstract/\n",
        "!mkdir ./data/landscapes/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n",
            "mkdir: cannot create directory ‘./data’: File exists\n",
            "mkdir: cannot create directory ‘./data/robgon-abstract/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_jiZ_rqfpfS",
        "outputId": "3c4d6403-e6ab-4115-c8b6-d1cd2b740964"
      },
      "source": [
        "!pip install kaggle -q      # At first, I suspect the kaggle API lose effect so it doesn't have .kaggle folder. (not working)\n",
        "!rm -rf /root/.kaggle.      # when I created the folder, it says the file or dir already exits\n",
        "!mkdir /root/.kaggle        # successful\n",
        "!cp /content/drive/MyDrive/kaggle.json /root/.kaggle/kaggle.json    # not sure if I have to use full destination path, I previously only used /root/.kaggle and it failed. Don't have time to validate this thought.\n",
        "!ls /root/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root/.kaggle/kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M119p23jf6mK",
        "outputId": "91fdbb08-4c4d-4224-cc5a-ba3a3a5055cc"
      },
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "#!kaggle datasets download -d robgonsalves/abstract-paintings -p Dataset/abstract-art\n",
        "!kaggle datasets download -d arnaud58/landscape-pictures -p Dataset/landscape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n",
            "Downloading landscape-pictures.zip to Dataset/landscape\n",
            " 98% 609M/620M [00:17<00:00, 52.8MB/s]\n",
            "100% 620M/620M [00:17<00:00, 38.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOq9R0eggsvj"
      },
      "source": [
        "#!mkdir /content/drive/MyDrive/Dataset/robgon-abstract\n",
        "!unzip /content/drive/MyDrive/Dataset/abstract-art/abstract-paintings.zip drive/MyDrive/Dataset/abstract-art\n",
        "!rm -rf /content/drive/MyDrive/Dataset/abstract-art/abstract-paintings.zip\n",
        "\n",
        "!unzip /content/drive/MyDrive/Dataset/landscape/landscape-pictures.zip drive/MyDrive/Dataset/landscape\n",
        "!rm -rf /content/drive/MyDrive/Dataset/landscape/landscape-pictures.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6GyOw_hlr6D"
      },
      "source": [
        "# Landscape dataset Preprocessing Images to 1024x1024\n",
        "%cd /content/drive/MyDrive/Dataset/landscape_final\n",
        "!unzip /content/drive/MyDrive/Dataset/landscape-resized/landscape-1.zip\n",
        "!unzip /content/drive/MyDrive/Dataset/landscape-resized/landscape-2.zip\n",
        "!unzip /content/drive/MyDrive/Dataset/landscape-resized/landscape-3.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6leDzGXwrXal",
        "outputId": "8ef607ce-64f5-4d2f-9e7f-ad21dd718bf7"
      },
      "source": [
        "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-10-27 21:52:09--  https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl\n",
            "Resolving nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)... 99.86.38.107, 99.86.38.69, 99.86.38.33, ...\n",
            "Connecting to nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)|99.86.38.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 381646055 (364M) [binary/octet-stream]\n",
            "Saving to: ‘ffhq.pkl’\n",
            "\n",
            "ffhq.pkl            100%[===================>] 363.97M  30.0MB/s    in 15s     \n",
            "\n",
            "2021-10-27 21:52:25 (23.8 MB/s) - ‘ffhq.pkl’ saved [381646055/381646055]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeS9tDvt61VG"
      },
      "source": [
        "## Convert dataset to .tfrecords (Must Step) \n",
        "Model would not work Output should be as in the image down below\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAscAAAB5CAYAAADVqYwCAAAgAElEQVR4Ae1d63XjvA68ZW1Fu43ka8NFZHtIB6nH9wAkyAEIkvIrdrLzI0e2ROIxGIAQ7cj/+/Xr15l/xIAcIAfIAXKAHCAHyAFygBz4df4fQWAikAPkADlADpAD5AA5QA6QA4UDbI65c85PDsgBcoAcIAfIAXKAHCAHKgfYHDMZmAzkADlADpAD5AA5QA6QA5UDbI6ZDEwGcoAcIAfIAXKAHCAHyIHKATbHTAYmAzlADpAD5AA5QA6QA+RA5QCbYyYDk4EcIAfIAXKAHCAHyAFyoHKAzTGTgclADpAD5AA5QA6QA+QAOVA5wOaYycBkIAfIAXKAHCAHyAFygByoHLhLc/z29/P8cfr9b4H653T++Hw/v82S6b/38+fH6fx7dj07rzI/z59/3x6I5e/z6ePz/P7f6zzPUfjz+flxPv15oE3XxCOLkZ57O79/is3X2C1zH+zr1O4H4jvovI1nX1lTfp8+aiwfnXtfif+Funb1bIjvhfI5/4E1nbHgs3nJgXtz4Pbm+MuL6m2L7g7Ao4vydtw1zdhXNMfBrtKYQpNfbThys+OaitosXtN0Fxse2TA+ijPXNLrXzLlf4StY18b+0pu3ixqcGzC/pqZcMyf4o3x+6I3p5XH8snjdAb9dbeX1y+NPzIgZOfAcDtzcHMuCcqSRul+Ab1h0w2KY2SSL0d4faXCgoczkhiY00/WMc+Jfb2AFy4/zx0dvTDWecu7AJwHaTGCDJT5/ovznkHrAVRZ+tDOL11Xnrml0r5lzJxwlPq35K3nU399JR8Px+jy9qqbcoblTPjd87o3HFfK+Ml53wG/Iu8aFK3znXO40kwPkwBM5cGNzPF/odaFpHz37hml+rTRrpz8i1z627k2o20Vp10MzWxu07CNvW3RRjjWK0aYyP/+YVccmi+ggAxoya7pxjOmWRQVt8o1pwQLH/voVz9VGxzBJbNOFa1gAK96n99oM/z6f/p7Ob/Ixs8qIemSR6+fUF/BRdLhzTV+Zo5jieL0+xllt1Tj22OfnQO5n4AEklWGvMuD8L8eVMN/ZlvOgYNFvLFA+xtl/ZURs/jif/pOv5Vy3g+tl+9yy+DT+qo6FjRgPxCZ9HXg2zPXxEBscbwPe7lrTV/EZvmITZDfd4bxh2r66Uq57Xdm5yt0sd5zdI5aYu4Z7z+EVZmLH+/kNudb8GhtKl1sNr3Fc52HAJvrm/BIu+nzzPBv97npWNvAacSIHyIHvx4GbmmMtnrHgWoMUCm0jhxZkKML63gqvLST+fV9oBOAyxi92FXhdZGzur3NpfrouK/YmL1tspo1UW4xkwQEddj7zCxa6toBWvDLdglGmX+cizupn98tfL/iYjw33VLaMFV/ezu9i65/T+f30uzS4VZ+X7TFNfUAc2qJveOVNya/gT7F5HKv6Gg7Rz3G8ykllmx9mV0zcKKu8HzGV84mMiqNh73EqsnojEv2ItoT3M38qD128Kv5prlieAkfN3tnx9+kdvhce7c7fd91v5/cWO8O/c9h0+hh338WvEf9+PedQue4wEZyQo5a/hgfaKNcUQ4hxnOveFwxwN36NWeRCed8xA//Mvgvi9fYX8Q2yo1+RV0sOe7ssdjwSF3KAHPgpHLihOS4LwVjIQxGGxcca27jI9YVvlDkuluMYC4aM9bL92EFWXBDSBjKQXRbDYYHyetSeME4XaJyX6JZ5HQvQG8Z6PwVvXATr4o+6NAYyDhb5cO7t78f5/W9pfhxOQTfap+OiHhyvr/3uIc63uM0aG2eH3hSB/SI76PbjC37ZuRkPzZ7UrxDPMjbDFOJm3EdMdOcd/Li06UkwNbtt17jnZOGlz4lq31JO4oP5AkeH7YBPkhMwN991n88Z8sfJsiY25IGNcfhPcsziEJpj8dHj521Uu2BOyh2zY9Ax8ifND5l/h3ihbHyt/AkYdU4hXyb4gn/DPF7jR+PkADnwDTlwfXM8LIZYRP3i3wumX1jsfC/U43W3ACvA4xiUYx9r4tGahUFWsiB0W7IGYaY7OR/wWcvtuvJxKF9eA7510UR/9fWBxtE1KGivvG4LPur2jXjaCMhca9YTfC1W7jgbh+flNfqkeuwrGXBsdgumY/NR9Mp537SjPQNPhHeIT0v0mfyCmY+JNRbJnCBbOIBzfXNmO591DGJSP1VpsVP8gCvO7mwndrTbckfxUXnetqYr+GA3IDg/+uW/bjLD2HLD24Zyu22Gsc2x45zDu7iPNhf/Tb9yxfhe8XfxWmGW8DPP/8JXJ1diOcgO/ic5YjIGPSoL53u8Cx/xumHLI3KIr8kHcuBncODq5ngorm7hzRZkAawUXCvQhUR4DhexAvDYqIxjjIxzmyayhgVhvqukOpLx6IMtmHouNAs723Y+NBzEBmyIpjYhQWeYyaKbLHjB9qZbzkPzqefRlrrz3XbWD9lmi3xiR+WL4Cq6HG+CjYafO07HzPAAngS/Ml9nzbfEumHQmhjzb2yOG74thzB269dRl75v37vNGuDSaGEcHWZTG8YGzdk9YO0x1rGOayMOR3PEmkKXbxuuNVsDh9H3NgYw2Nok8gBvF/f6/XzkrddxBIMr46V4+BtA9AVfKwYBP7nufAnXETe+Xuco8SE+5MD348B1zfGmUA6FFRabYZHUxcUaB7+gCqH8YlIAVvnQpDXiqaxZY57ISvxQfaExMvnDggJ+OZtUrl9cVnNNvhyn40TmR/lnOdcUxN1CsKnJFVxSn2ThNeyBvMP4sojjUy1EdsSqxBbwT/BtNqGdi3Ei8+P0dj59RDtL04CNh5c9cgmvF1ujzIpBjV/Huejq7w2rgkt8PrPjgsWn4RznzGSbjvXRxWCBY/G96Lq8MRYbgp0VoyZL3/fYKwawO+/stJso/JrP1nbEIdiiXMrOxTn+ySzIB3mtNsa6sqwpa45tMdvuHN8QrxAP++dTyxcfj6qncbTUoRbbgcOIK19HHvE9OUEOfH8OXNUcp4sINjtt8esfwWJjofPbbktfUG1neRgbF6y6UNuOjRV8JaQuZl1v+4g/W/zSBbkseCa7LxCygKCtMfi2wIju+h/o0JBOm15Y2JvOio3zq2GaNXTB5vDkhqnu1P/8421tdsAfwdrHsfqNPJjJr2PG+YkMi/XAAcEfMS8xb9zZ6M7sd3jr/CJT4uKuDRyTccCNMPf9hD8Ys7AZsZu8HjHzfLCmFLlkto9zA2YTnVbo/fz386k91aTkAl4XnWJLi0eI1cfp5PJJ56YxzuNsPpltenRxgXhUvzIOW81BvPQ12uLkCmaA+XDN3xQjJjLPYzbWFLHRfPNzOxc7prEG+feOCx+n8wlke78FK7EF/Fpy2OtxMdhwiGOJHTlADnwHDlzRHI8F/Ts4equN68X7hcl+oEk8gg0u2kfGP3vMd7P3LnhlsdZzY6N4F313a4S+pqbcnxPZznE5Zw3ua+H8wnXqblyij+QcOUAO3M6BK5rj25UycN8MQ90dg10lLmSv+d/HSZzK7iNjN3t82221SJp63B2378+Hc8yX18wXxoVxIQfIgQkH2BxPgLlt0fxmze8Mg/aR8avvPP4QvGdxuOC8+yhdv57zjzfGj+Zwk3/51x5YY5i35AA5QA68JgfYHF/QeJDEr0lixoVxIQfIAXKAHCAHyIF7cYDNMZtjfqxCDpAD5AA5QA6QA+QAOVA5wOaYycBkIAfIAXKAHCAHyAFygByoHGBzzGRgMpAD5AA5QA6QA+QAOUAOVA68fHOcPn7JnsGJzyL9IlLbs0ePPmv0Xt9/ebyc8KzkJ2A7+Kj/7FT+GXD/1IXssVr/wPevBKPw/GnEMc2fTa6Uf+p78D9hZo+d29iFft3/dXnyxArLe+i8Jh730PtUGTHWVr+/4h9GN/nxVFyu4nvlafos/Mdw2Na84fnf1X6szVo7FvXo++H9D6whV/Hw5+NyXXP8hQUnXUysuD6hgbNC8dOaY/Xr1YraJc3xF3JSC3xc8O9ZYA7L3t8QpPmzsVUXOPxRk834axY8Z1fNZ3w2cLHhyJM2wk3d1Q3XYxoLh02Iq9US/AGSS+uKw/EBcXL2O/l77tncqY0BDxt/1+NX1wWH0f0bCM2L6br3WA4rXxPdhcclV9W+V1tHHhyTu/KVtr7MzvX3bI5JoLsTaLqAPRNrXDw3i5zYf2ljcVNRQ9vujdFR2TJusxC9ZFz11/Kg8dXYfpw/2sL7dn7/+Dh/4C+2TTEuTdp1jfX9m5cVp6SJQDu1qcD4CQ7xuclTv4vtz4vv0eZYGjaINfpzlOc459LXm7qxitfrXTuK+WN4PWuO9RnilcfTMZfGjePvvsa/Hp8fw9N7+fmY5liLXn/uZ/8J5gpGXQRsx8Q3NaUA2DU54oIii4Fdw/MKiMj9++Z+1jiOwflTOWli1rty3ZmKHzmXBUB/GlbtfTufPsROHBf8wkVR9EXMwg6YFp36UZr7+Vq1NcieLUapXxKTMj9i1TAVW9G+1sQkdrdrIvPjfDqVBV/iYtj7eN8hQaaLLMbM/6zv8NPTLh51QUef2/Ugs8UEYx1/Whubg9tlW/LnjVHkAuRPwwl8aH7FWKLN9SfFI680j3EcyA15azbLcVhANW9P59NHlfXf+/n9P5FVMNXxaKfJ0HMZd8M5kS9jMZ6NpyFWcD7PiyDbbGk8QDyQ290fw2Lt17pW+nrQa6KvtT4e/prFtM+N+e91dL8sj61+2jHOT2ONNajxEXGS14HDLvbi0/v5DWPprod4Slzc9SA7cnqIZ7jpVs53zLCWCV6CAeKD12PNyfAybuTHEk8vs2DnYuU4LNcvxczXMrNFdQyyY+z43vDi8Xtz4QHNcUzg8r4XgrfzOyZYWGC1sMD1vAH4pQWoy6xBsMJl84Ns/ytZtUjaWCzay9fiTywexUddfKoNUsDQ9t+n9/Ppj5ElLrDhvcoAHcEPLVJQ8K8vWtXutrBb0Y+65XxdHHVRsutlfi/W5X2JS8VX7KwLmZy/3lbDbjwizr0goS1xTsDbFuPGBcPFGoLopzWSdj3IX8brRtnGzUljscyf1lDM4lf9SGWPGPhYRkzH8SU2ct70V32CV72pLXkjuMI4tQfnoC583ePgcNB4zDjc53h/ynk9B7lWGg2wZRnrS2Vbw2q8WtdKwTPnftH79tfkyPsYD8DXOIXHrV8F95773Vefg4AVypfXKc+kuV3VyuJHq0fRr2A37mqKXVmMu73WWCNu6Nc6HiobbwqdLZGnMR6oJ7xueWv12Y6jnbl/G8z+nM7vp99tp3LkfLEnlx1sjTHm+4Yr8oyvX583d2+O08SSIuEWGAQGinRSLGfFPz0/6AHZdSHB3ZPU1m0ye5mF5HAObEhtrPJdoRkWf5HXdyxEjl+EQJ8VfGtet/Yj9vY6Fm47b4s1LnB9UUzxa/73cbgIOr+vshVs0/keCys6qW2mT2yMeDnujTKHWLrx3qZ1vG6T7fxrzXzVn9jk7NbrnkvuuuGTyBG9PnYSX+CFzAk57sdXGxs/OmZtnMj4+15vnj1Ozk5nX87dJlN80niDrfVmyOdU9M/s83aoLMB9HWuTAblgGBueATPMFYt1PwZbNs1xn1fscBjWpnLYYKj27f3KfXI6k1i76y6OhtV4dLHEmyawFW/IXVyDDSor5n6LieDr88PZ28aZjT4e3k4ZA9eP5segw3SZvLV9ow02D/m/vqmacTCXjfbx9ZovxOc74fOY5hgWDwUjFCgpvPZRXDnWxE2KpS/onVzp+aDHFSdbjFphzBfVffCg4LVCBufABmej+hb8bjjJfPj4W2QEOz1eImcsdjZmtuDNfVtgAf7E+WmxbONh8YS4pnMajj2+Udfs/UyecqzhG+Q2G+E82Bh5I7pdLMVeNx7k2C70dDceuFL9Pi7b9IwyFJ/EJic7uZ7iOhuH5+U1NnbK2cBvwcDFADgBMe8xxOvyGjgOMZPxneNlTn9fMHLxh7mpv9WWbofh3GWZfJHbm6+i2/KuH8FukT3Rr/oQQxvbcr/wrsvN895sG3xLYuLGajwtZrgLecSvMqZj4TGzr0bMry9yyNlV7Ws8GrnfOZ7YlGCv3Kj5OeIRYgc8FXxxbolLHz/yp9R1xSCJhc5vfkX8svcgL9hlsR9tEDkrzOR6Fm/kQ7Ell53ZyXMWDx6/Lxce0xyHgo+LgL6G4u8SFxffmvy98HmQ0/NDIQxFIRaoYOcxIgeZaiecAxu6jXIdml9r1FthHItTX1SSgj8pjMX+Udfer6LDLRSmA/yJcjCudq2fA7shrnq9+e1jajKOH0GH2VuPSz3Kg1D83TmIZ5XXY1ltBp+8vXObyrhbZFfds5gkNjm7k+ve9uO+Cb6OLzObMC4T/YMsnRNxNNzkfG9IbGF3tsRd0SO2DTkJ3GzzxQbkTbQR5oDfLgZwXjkaapCMtU/Z9LrTZxh0PTPZ5eYNG/nkBi/a0nQd8WszZhJrx7d0jPi4qpUrDBKbWuw6Zt2GqGuU3cdmX7nw4zVerrbB9aUdmW3ZuWJvXxvGMaMNMgbsqDFH3iDn1N80LrNPVkYbEDO+Jj7flQN3b47HouwTOi4ImphtFzQZGwqlAY3JbefGHRosCknhhMWhydieQ5lGfDgHRbDb6P0yjNquGszJ7BgXSdObHYufvlnIxuG5xZyVbVpEcQFGPwFvKLZ58UZbLnh9kW0ot9jZMYr+QzyTxaTECH1F2dkiitdvk23NYL5AepsKb6DRgDhkPGvnFuNE5sdJ/uEUm0Txr+jumKLP5XXPB3+tyOzfeSx2AH9qDNQfeYqFa0Bi7Mp7azBV1oonkO8q38k2O8W39/Pbf+/+hsAa6tZU2ng4brBEO1V/q4WVR9A8+1pZdBRMTuff4If6rHrhJkIwmNTShhH4UWyJMQa/bBc1xWvdiJf4znaOPYeHWnmk0TObFIN+s9H0Nqwid+rOMGCOcyLWMR563XQbPk3WPj9QV/46YNP86HGJNhQ5Mg+4ED4JUz+a3TV/gAtmSy6767ZxPBKTn8CB65vj+LFxKwJW9OzjOlicNZlLgttHhR+nk0/cWsTtIydJyL7g+rldRl1Yh0UwFAWUbfaj3UmxaUHO5raCA3rABmwGtLCYzs/3sz7ZYihIHTPxDRsgPx8LvhUzmNvkHk3ScZFwfq8wsgVo+JiyyFQfoEG4Z4FFfJu9GMNgGzYi1swZh9qNSuPofDFpuhwn/Ph5vIAr1dbUj5lswLLZgT7jvPpPbi1/NnMHmzWmsUGqOZhyrF5rPEcOj36b/an/2feCazwxL+xmocVRdEfbICdNZz8m+ZPIKNj4GJuMATfIF70W7anxGuYNDYnHc6iVKifYD7oE14bLx+l8+gu1GHmi8Rp9G+wDv4rv0T67wZnHWuYNclV/55m/HmvlKNvzB22qT7VodgeskjiLfQ43V4dRtmDp1y5vN9Zoq8N+fqzxxqf5scz3/BfZiV/ON5nn4+swC3Xy/XSCxyjuZJtvPM7jRmy+IzbXNce4GH+b1yXJfWEp51rz8CxfssVbF6++YHxHcj3U5k2j91Ddz+JJ2PH5Tj5q4wCN21W2f7uYj03JVX4/kW/X2HuXWH8znwWnf9XvazjCOWyYX50D/1BznNx1pztRX09aLaptd6Po192LcO7VyUT7vp47/w7mL3Ij+w2btn+HI8/NPzbHz8WfPCf+9+TAP9Qc2yOd4KNG95HZM4mVfHTFxpjPh2Qj5j8yvnXnmXgypx7IgWubY52nXy3xa1P5Soz/OsQ9F3/KeuaaT92vzr9/qzl+YGF89UDTPhYjcoAcIAfIAXKAHCAH9hxgc8yGmbtJ5AA5QA6QA+QAOUAOkAOVA2yOmQxMBnKAHCAHyAFygBwgB8iBygE2x0wGJgM5QA6QA+QAOUAOkAPkQOXAj2yO7R8c/GPb9t8xuev3cLLHs/3AxCtYl0fO3fyEjQ1m7tmcPxDLlH/x8WX1CSvln3Ue/Ki/TTxSe58Sl/oPrfoPe+WpNE9/PONTcPjiGkcf2UiQA+TAD+XA9c2xPofX/rv2K/+jtiyEq8b3ezbHe78e1ozEBuwCst+vOd77f3Fz/Mzm7k66pz7fELPDPLqTD4f1XcA7L5PNsceDTTLxIAfIAXLgFg5c1RyXhggb4t/n09/kJ0yvXuxWQd03UbcAcre5FzcWT/TrlkYL/FReXPu4LbFh8/i6aaM44xnYdre4znTF83fRLbugk93hW2IWbZ29v4sPq1y+37XOjSfm0QxHnufuGjlADpAD34oDVzTHyY9puKD3xUkWrPRZjbLotuc6YpMti2WZ365Dw9Tlmdxy7B+hFttSnfUXjGQsyvE70DjfdET75gu67Vhntt/mV/zJ1WhTwGxoqLxfOV7mrxyj/LnP92o6e3ODuqJf8PO3yjnvV/vZYG0c0R97jY1mkA08Kz4F2fH6jMOHdG9k13xa3mxMm+OFX2Jz/Ulp42jnQsH9Ig4PPIs8xZ+PHp8z3nNPbBbOIS4YK7ENr0UeIGf4+l45STnkEjlADvyrHLi8OdamIC5cSKC+ONvCq82o7Sjqog7NV5D3+/R+Pv0xeUWWySlBKuf6wmpj8SgLKejAZuOzL6zaCEDTM9oZFnd3E4D6bOEHXMQvkH2TX39O5/fT73bXFe1eNlH1ZqNjWJoMh9+00Qo+rvy/5dpEv4tH8tPJb38B79o8Ob9CDDDJd/HIm/WKx4bDqmeheym74ZhzuPkwwWzpl+bapzbIzUZscPU6YBp8WPPMGmOY33wR3N7O71YD5LzTZTWj56yPfawDCYedri/iLXW2mtR4SUyICTlADvwADjyuOY4LYW0UZYHtjZosYutmd1yQ1+NLkc4bi0GWazDighsX5NWCm9gUGou4eAy2bHBw853dm6ZExkKTLnIG3UGe0/UFJB/sEZ2JTbumcri+iQH6GW3Q5izgZuNl7JbDC90r2aZDm8eJfh2T4NPmQsycX4NNmCd7DqssbKZBj+3supsTdz3mzwW6j3B4qSvq5vuMKzxHXpAD5AA5UDjwsOZ4tkhqY9C+UmEfd8MOrS76/bx+/IuN9qEmEhfeTnbXKMhi6hqM0hz4j+b7TtaaMPvGoui61q9qm8PN79Ahrq5xk4bIzas2IKYOh47X2ud7jctj5WNTdImPO9+G67MG8yDPDDvkM2Jt1+WIY9bNrY+nm6dNXsKn2PzNYrby68bmWPiAvjusVe86X3Buwc3GJ/4qbyvHj3A44sP33L0iB8gBcoAcuJIDlzfHdfEdF3RrlpKFDowbGhy4ZrtPuOgODe3DmmO/8MvijXasG8XEZ9eISAPo5V3ilzYV2OTNGiPFMuhydliMwnEpL4x18brDtZl9iU2OOwkP3XWxcya7fgUD4zvGA3wLugY9GSZT3SBX5gXZyrPE94F/6ZgQ+/gpwWAT3pjsOBzsHjBEWXFs9ukGjt/oHuwe5Q/4ZDHhOS6U5AA5QA6QAwc4cHlz3HaPcOcSn1aRLHRoiCx0yfeBy+JWFvfWeNfGoe3mVjnaLOLOJ8rX17jw9oV0aICwwcDXg7wuY7YIO5vM7tbQ3uaXk11vDj6nH28X/HvjV3T395kvwb4r/J/hIufV/rizqjpWXPE2aezwBkMxtp1H+w6rvwEpjSeMaX552dagRp51n8L4JYcrvtG+pjviH2Qn363udsDclK9BlvHQcmVoMmV8x8fxzOY2DoNuiB3ySudPxmv84FrhhOmOPCh+dNnxfbSF71OOTDlHvIgXOUAOkAMrDlzVHItAa3j8x6MCdlzokgBoc4FfMeiNtjVBRe77+XT66P9A1Ip9WSzLGGiIBrmiwxbg/XdtvU/Fvr5AJ340e+Qa2vR+fpPmApqBm/yyRqV+PeL9dDp/tOa44G1Y6NGaoWYf2lb8ajcgNsZh1zFbkefoNcN10Kl+9dgP8tCm+pQFjIfJVZ8/TudT/NqF7Zy2r5V0Xet4jHihXrUTbVP5Xbb5EXW8KdY72XJ9jr+XaTnUdfvrIX/EZuBk4SzqQtsih4/wLNYF/KoJypacPYGfo+wBb5dfEw4bl3nkzhA5QA6QA+TADRy4ujm2BuDHHLNGTc9h87BrkHn9Ej5Iczs2Qf82htrcDjc3Px2TAzfUNxS5SzjJsT+da/SPHCcHyIE9B9gc26KrO4F9B07IU3bh/DmSak8qYkSMLuMAm+PL8CK/iBc5QA6QA4/kAJtja46Hr4rIR7dsjB9JPspmcSscYHPMXGAukAPkADnwOhxgcwzNMYn5OsRkLBgLcoAcIAfIAXKAHHgGB9gcsznml/bJAXKAHCAHyAFygBwgByoH2BwzGZgM5AA5QA6QA+QAOUAOkAOVA2yOmQxMBnKAHCAHyAFygBwgB8iByoG7NMd8JNft3wnCZ/by8Wa34/mM7yjdU6c9r3h4NvQ3L94/1a97xj7Kejxm+T9ESk26hX+Pt1vqRG57xPBfe881OfldgxetnZon4fGdcu6mPsB+GyHI/U558GwOP6c5zp4p/FXEHX4I4YJG7AvsfjYhsuQRm9qPjOCPSAw/hFHH3SMhE9k3FYuv4tchPfsF/e6NxYy7ch5jesj+C3ImyLu7X0F+xt8j5x6Zd7fKfjRmmt9Zzipnrn/O+6PtLnHd59KR+L/EmFmOXsHxWznX8EgecforO3eFjU3Hcu718VX+Zbxe6oPadsd4TH2dYll+NOnqm1O1/TP5ATXw7ygObtz18Zhi4OR3++7G4Yn8nT1sji8B7guS5dmEiITRAgPN03QhVRxL4tyliY1Foyb7XWRfEvOHjP26AtPiqfgljY6ch/i28Q/xuxe+V9PzyLx7pOybcYx5FuMu11+aH0/IpYjRvd7fcX25G+cyfmTn7oXBIOf6+L5+c7xpgO/Ih5vrRIvL9fG41Ia7cbjZftn6c2VzXABqu4mf8ZfO/E/Ffra7t3C+/awvLtpB9lCYg4x4XRPXdjpBrhLNzuPxyLOMg87U7vjTuaBbgxNkRLtrAC8nhOAlulB+9wmCMGQAAB69SURBVCk2t0LQ6bnhZ4uLTHf3ukrYyTXVN8jeEDUrwMM59NlzUHRKIy14Gk/RD8O52FbG4HX/c+BedknywNPwTGyUiz9hjvaYXXLsTT/6FDlUMPMycMyKCyIXx1b8BVPg4sxu8XmNmcivPzlt+QFyPZ7RjpXd1c40fzvPG68zHxfF0fvbudJrlv0YUL/meTK/vpddOCTyekwRmxUX7oFZ1184neXkkTFx3sruMjZiEzGd2yPzUf6Yu142cmSHWfc1j8evc9k1NS5grAyDYNs0B3BXL8yx/AlcnvsluovteU0x2644DjXXMEBcg/3OZxu/wiy3q8fA5pZjr5Xz3Gv14O8b5BZ8TWjpV/DnLvFAvIq/Gs/WG80xQH/XeVFkIG7DXPH771v9gbMRz4Zb87ljhnKnPFNcLV5jfngOx+sP4vCi/q/wvKo5VpAgqPIeg/D2F4lQiOaK36SJKoF5P5/+GFEKWF621+WcU7kA+CwBYvIeBW9p90fSZBgO0Y9KAsDQ/IhY2vn50QjV/XbxiZjUIoqYiuxC2i6j6JPYHTnXkzLKncu2GE+OWeycLxFTzzNLwmZPkKcYSQGoMdDxjRdr2W0hSuKnuP05nd9Pv9s/NnjZ4m+R73Ji4GCGfY1Ts9Pi5nnWm3H0A+QJjiZDcDE/NnavMSv49x/O8fEofBLfwY7mc7Ezt7vj5WMZeWlYjOe77gnXoPEfxgbelOYIdCxqgsma57T53Wuay92Gz4MwO2C7+KD8NY40m+ZYmt95rH+dfx3U2+WgLuR050bLpRAvn3uGd4+fx9uuT+Khdve549cLCucbTwNWt6yLUZf3q95cQYzmnEMsD7wOeGpMwrmlri1mOxtKTFp8EdNgR8xNxQg3Htz4sT55Lmx46mRN6jLEY+Tywq/oo9VqPH/gdRoXtbuve5FX+9xc2L2L9ZH1BTBL7T/g94j1jmP59cub46Sw7ZwYricyZg7FwiyycKcL58lYX5iSQAo5riTbnDhjorkGKCSS2jzBYMBqS4a9j07mRC/iiK8V70DY3sQAqS6UizrS1xlm2FiJvhBH5Aq+LvJ9gzHwCO3fyB4Kyi5GKFvHJjEbZHh7uw/9Tr6cQ1n4usSm49Cv6Tm76RGcIb4uFsHuJWYYm+qL413zL/Or22b6u922SEFTUncO0wWz6QFuHjiX21rs8jWl76CrrYpRjInXncuWMUW+wz+tTw/CLNXlbVcfj44bcM7stniuMTMeDMfBFs8dwdrzAm3wY0ff1vEQTnoueHnK2VCTBvsBo4EXId9w7tKvZN4gG/Si3O1rwTt8Khbrn+iarak7zLb6pxsJBXsfD5+broao/8KFzg9/HXlScyDB1exdxsNuKCNuLgaJPnd9b4PZMjumHBjyJ9ixrWee86j74lgjvvi64pDan2H0oHOPaY41oWxrPdm6T4BoINfg4La9WzxsQanb/lgINUnh4wCTgWM0sS8oYM0uCcDU7kAwDRaQaCDkXNblhAA9RpJY0ED/SOBkMTQ5ehTfeizf/5P3tlPZ5yr2sybLyetzHLZxTPRBriv+tUnS690ui7VxxRc+0Vn8MC4scT4ie8mhEpNmk+KHmCUxi/4nzWa+E4ey8HXBuePQr+kO1n/vutB7PpQxM7uXmCX25uM3uVJx6Hb32LWFMOPGgN9BntV5ua0dM+TqMBb5kvBiGN9szeWjrvL6MZgpxom9g/5p3dthnNld52wwG2wwzGSesxkxzPgrNcJurHAs2mG5mVw3vfXTBZ8bpf5gTbH6k9qPPtea2jgteqY4b/xK5s05t4tZuK42Gz4ZZnLO22d4CAZixwqzFCfA3GSjzDInjxX67WtIt7PJQtzEz7h+4fXEptEv41nBCX13cW6x9uNzLBY55GyqsYFziEWTPeRPIh956nItwTDoGzHpNyMWSz+mcivBOrUf9DWfHnTu/s2xOomA+Ls5dSgBojgqgeofacm5keBAgqDrEJgDOUDeDuSN3S3pVE7xRc8p2XYFpthxyAdnZ1IkBh8tAWTskYRcYCIYxIRRXG6U63yy76l5zJQL1pgPPnqbR94YBgdw3sje3WBJDN0CPvAmiVn0P2k2Y4OPOVO4N8pFHMQuvbnRReDt/P5xOr/BbtjO7jU3Pb5iWz5+HGdFE/MH7bbrWFRxbMHBx//Sc7mtBU+/sGXnuu4BwykOMmeMV273gzDb8dw4eXScjW/HzO6OlfmaYWbXhuNgC2KIr0c9Kd5O3np+zpGux3O2n1cftAZcuy6u7cqa6p2tA64tZpndvg4v61/w83Y7Zr6X86vcHOMR+dhli51DTRnqtmHT5x3DUfT6vibf6DD5cJzaAGMmsUuxd3wXGRETL1dkuLVsUbNSfWDbIAt9w9d1zk7eMey9P5fMubw5VjA7kZSAGHh1EholCQZeV8cLWQYyBtkl6eH7MQB0cTLIUV2gexhvd+ebMdm8pd31DhmaRsWlvY/JkSV2CeKcEGVO3wWxoMdEjbrKuGLPx/kj3h1XX0scN7jE2Na5SvqJXInTIdkRc40lFOXKo86Z3E8jv+oEm2JiznEWvNayd9c9HhY38MV2VMA+s7sfxYYxHtGPEtfT+bfiF7ngby7V5w/7aFjGfpzf/9r7yuFm02j3HjNvbz4+82tt93IxBt5cxbM63+NouWXchdhFXoL+xvWW95h7FqMuO23WgrzChwdhlixInX/dzphL2Zj8XGZ3l2tzZtjbdXcMNUjzIX5UbjfQA5aRZ2IfNi3xerB1t76obX1tXNmtnHa6RVexp9e4rr9wG3jofPPzylj0q8qpNXTYIXWyus5iv5dtnPVNKc4J43eYLXUXuRrjVpe6rgGTkJt6Hebp+5CbpbZ8jBs+alfwBWwddMM1F3c9X3jlMdtwzeSJT9Fmu7Y5pvV3kCc++rqN9meYzeJROD2X5ecV//tXdjzWBd97cbhzBn3bvb6iObZdvfpxSf3PRwy8gmAfxX+czqfwD3tqlBLZPnLpgBooZZfo/Xw6fcDHHQVA3EFCvaNckT8WlKjjbUMyB+LEbhnj/B70Btshaa0ool/jDYWRqWNV7LLzhmVCKPFvVbgXDewWK8Uj2uTJWGSsxziMxV6Hs/iWzQ+YDotkxyQWGInVwB3Hg7nsYmu8DjyrWFs830+n88eGD82Wwe/R9znPxoKr2FeulXkdR4ttW4w3dq8xEzy6bMuHY36t7bYF2fC0Y7O7xu0qnrWYhzyC/DScil7vo78msQIebGWPfrs8WHJhnKu2NLuDP7Uee8zKmBajZi/m76jH2ZjNWdptNxyQmylmaIN/jZiL7cJL9Auva8xaczFi4n0/4OvgW4h3yCGsOy5vL1wXBfO5X6FeJmuyxsxsbxzxuE7jGn1y82MdTOqq6bWe4MJ4x/URY+Yx2eRm4wH6XexHmQ4HZ/tR+SPPshsStT21qdu3rrl9XLd5jIfkQPNP/HE6fd32eM7qmdfRZEstcHiF+YFHw7qIc+/N4axObc5d1xxvhPZAZcHjufvic6CgS7yUmKGQ/+A4apK7Ik7e3Zd3X4jnUNCtCP87fL44dkcx0wVpgWMm59vWjYO18tv694U5+WMw8s3hxXl2Ew6lycQbO6f/H1uzne834XqfPGBz/AJBuI0URwp+GePu8L693+sEYHO8xuc2zn2t7GyHRXfh3A7I19r06vhdgplimd1I6uLsd8te3e+1fUdqJXm0xvBn4TPl/letj9Ob003j/FX2/cN62Bxr8P3HBPaxbTtmC8fLkGZV8Ms19eOlfbh/wWVzfH9Mn7doAo/to1k2xu052nlcLsEsryHSOEx3tV6m/l3C89zPHL9L5HLsd8NQm2KpJS9QR7K1Ss79S5tZr8gfNsffssizGL9iMtEm8pIcIAfIAXKAHPj+HGBzzOZ4swP1/UnOQsUYkgPkADlADpAD5MBRDrA5ZnPM5pgcIAfIAXKAHCAHyAFyoHKAzTGTgclADpAD5AA5QA6QA+QAOVA58LzmWP9Lc/+f0PLF+a/9Ynr9R5Yn/QObfjnf/uno4udBPu4jk/IPDPt4Hf3I4t8ZFx8VFN8/Lmb3xthxc5Yf9izL2fWnLj7+H2+/tq4s4vzSmC3sfmgsa6we+Q9T935MncVR6/fi8XgPwq39k9mL/KPZvevPK8p7/LqY/ROp5MaCX46H45q9+2c/8Wn2j7iNY5P6fovsV4vvz2uOlRgL4mwL0+OaYyHWckFW20cyHyHNVvbW7/UiWJLiOtum9rskrj8KcM/F8N6L38UYFi75mN+hOX6yX9okT4qj/dhM9tD7KQ8qrjdx+EDeK4dndq9ie0D2zrfldcuDa2yb2W0y2432pvZkch7GsyM5cENzfMRuxefO9cwwfDRfTE843pQ/QdaSr5eOvQmPrDlcr1WX2H4rZlpTwo8fXaJ/N3ZWs7QGb9fKWZ6V3Jo1wKWGr3JjFZNbZd8vtjtsd9ef3Bzvm9iLyXtTIkpgsobmPgHb+nKkqE8K01b2ZN6OIA+9fnOsNnG5Ac97+J0XsFnB2viC8XsFv+7ZyFXfbuLwlkurgr7Bfit7Mx9j91Wv1WZY4OL7I3Y8jGd3yIGV/Vu7H1fjtW48iS835c8Kz1uv3YTHDXl7wO6XxUxsFx4vdoj3ti/ybBeTTQ4td4hvlH2PtfceMp7XHE+JW5KhPWMYf/pQ55Q7k3a9LdLhfNsxgQVi+AlOf+0IoNrwXHinWObgz6XW1832srjmzdR64d3L7sVFkqngFvzWJLRr4SMVJbpdizczgvn7+Q3HbO9mwZ9dEtUblTHWJiPE3PBEexoPxAezv2NiMVccbX7VK3fWxzALeDaeLs7/Jz8pXXFtmBW7/E4znNv6JbiU8VPMQqy9LsN1ffRY9bEdq8ku5UT3nsNdh8WrHwMHDNMhR8u46W5JWo8OyLZFBGPTeCR2h3i0WBeflpiJ7PozqhbPw/FSe5B/xY7iP3Cq+Q3n0JeGJ+aP2O6xGeyaxLrETeaibT6+jg8OSxvndbdn1R6yuzYdIQ5il+oN54dzwa+UT2qH1Rq02fsssXe4Odl+bOe7yRuPg7wWWxkbMPvsdV59RJwH+1ccFrmzNWDUma4/zs7uF+aG8V+OiJnjCvhkeM2ux/NNvuHQMAAfkBuOazHWgpfED+a2taf65+Zn62vBPOWX4dVs7JiZ3+Uo+uc8WvNlrV/wwzh4vWXdnF9fy46ynvX+5ZpjTQgj6K8R5Le/SMRCPkegFWH+nM7vp9/tC+eaIEh4I93iWJJqTrhVIGdkPFIEVnLl2kw2Ls5GVo/x2/kd8J7eraa4WvJbTJJ4LLDc/aS1t7MklPlQfDa9vfg7Lshik8Z3TE6Na8OhXMdC7GxRLIADuqihLfMFuC9SNt77NXAy6hI8p37VZj740TFbF8sdx+y6x2oszDkX97rzeaN8s8MdU37az6bb4oNHw/+A/Jlsi4U2kFVeiNfv0/v59Md0+Fij/anv1ixZPDOezfIr2BFz7TaeRT9i3u9ivbte8JrxLMUKcVjkh2A+nR8xG8beWiuhZkTZUfclsa6+T/2qN2i9DpT4Wa0ccFZben6sOVxi3zceIhcsB7s85P3+tbfVjY8Y6XvAOPjh5u4w07lSL0xe4pfISHUUm/vcmC/hfbR7KtfqiB0X2Kh/mzwTven6uM4/wVE40/lk9sDxBtlZnJ5x7rWa44Ro84QvgRiuJzKmwF4ytibTVNaB64OtYc6wYIXrK91z2TVRbYEVmUviThIqxWocO7cDEsf8UpnYsHzqTlnxU2SHgrq0O1n0puPHouIXiDVmY2HYyUPfR8x83P11b1eVM/Vrh5lc9zsvK07NrqU2WUzjot/O73VPuZPxRBpS5HTKz4h73y2b+ZaeX8mWWLQFVPSNXECZM+xS34c4e26g3OF1gpk1Q2Wsl5XaNeiveIrssKj6+btYe92D7ZUzXmaPpWC1WtTX9W0dHx8HsTPUoMZnsWfiR8qXcSzqEl99s+HtVJ/dLj42bgUblOcwHeLoZQ84p/Z3/P34tV9qx0aes9XhKzq9rX1sOe8xC2uA6l3n/BSzZG46NvVttNlhpnOs6TYeBTuHmHX8OwbB3wG7MTY4N2/sQc/KhtU1sSPF5aDswQ+Y94XXvl9zLEEJRcIlyDIohbR+/qr43TcoaXJBsDWBwqLjyAxj4/m57DFRs7keE0zcikGK65h8czsSLFOZqC80zhJ3xGfHhWkCj5i44jUtyMU28dHjVd5j86FjsHlrsRsxi4t5x1DsTGIx80vxTGxDzNyY67jvsRrj2u0P1za6p/MadkEenl9xSccJ7mEBwvmr1yvZs1iYPOdzjU3Ci9T3QXbCHdMTj6oXuVP8x1rZdV7IM7Er1GB9j345vyPPjvkx51nJX7MB805r24Ab8mbDA5wrr9GneuNnessRMa56Ur6MPnf8S5Pj5Y41Jdbt+B7luWvok/LE178B52i/i2Xk8NovtSPKi1xdvve2dr/y8wMG4rtxFetg1TmMN1uO2pyOG23zGBcOtlxUG32O6PjE3u5/4ZqXizyX12Ns/PzN9dQ30FGxbX4Ydnq8UbaTBTq/8Pz3ao41WH6BG8i9CKiMdc3VYqwn0X2CM9gaAn00ITLb5rLHRMX5qtPtjkxInWI1jp3bkWCYyqzjVtcEN72+4cKwKJgNIya+yIzXEbMjPnp5pleOI2bDWLNbfMwKpF0P/NnerYfxY+zRzvnrwd4g9zA+jneLXZAa67bI2WKHjcuOL/U7l0MjFWzHOLfXK9mzWKhcibXfqZ9hl2I2yB6502yMfqjNvnEbdJt8GXsJz2xe1Dl5P/LsmB+DvZl89dPXgXiz6TFa5zbmp8QE+XLYj5Qvo88Yc3zt7S15KNcH/rtPLBb5M8TLYzDg7OzfcXjtl/ri5M3rSub3ZTvHxa+8Was3IIHnU9yP2pyO8/iKXx7jch3jiTxTHIaY5bhN7ddcGWPjME5tBz0LG7w/MMdy9AbZzkaT94TjazXHYQErxQgWFwUcCr4ELyw+pbj5omZga4FpC6oR1N+x2djZsdgENlwQNJ0bkhP1rK+bvbnu+dwxUVc6SxFOdKRkH5MvS9YiM4lJKtMSrfrb4mXn6/EIF+KYFqsiuxXRyqO+S7TGTBffsDAhpvp6WlgiZvI+YlPt+/hwi3PTsfGr+xEwa/7X8+r3ZfwXG5Rrs7hMv1YRbEl0zzkc5kY/9H2GI87bXFdMw81z07OYO42z6A7zTEeCXZY3Y5Mn8kJumsxYVwaOBFvUt2t5VmS1/Gk4Id7weoh14kciY8ezkg+JX4PvYMsBfkos3v97O78HTCM/S10L8RA/VH/MK2+nysK1SzFKZCW4tDoQrqUcavZ02cXuXnO8X8XO6XeIjW+Nw2MsRzu874P9JjPgbePU3qavx7JgCDgPPOtjRZb3s1zLzqneNIZe3nzcuH6oHvNhWTOqjkP6Rz2GWTmOsXHXN3Y4mwPX5Noy/2+Q7WwMer/y2os1x/X7sLAzFINgia13XR+n8yn+x6+AqUlid9q9KNhuo92xvZ/kiQGQXAcCoYSJC9SBeSWohcymPzYxKntSIOwO2n+/EZN1JvtIAhlWciNycgtw8bdfL7YbZmPyjYWxf2Q43B1vC0DwCReT+BHnhAvefrPbFrDq18fp/Hb6gI9Qd5hFjokckK18mC0I5XzjwNAYQ9EeZPZ4T/2qXwlB+a2IubwQmyE3thweY6E6rODXRhD1yuvLdAcdTXb3e1kcnX/Rt1k8quzNAj2tKUcWAatnn+/nk+PZyIUBM1cPZHzwa2a3nW+6IRYQ68KjyN2O95xno+0tv10cMp6NczsXAwfM/saFcW7j2OBXze+YR2rf3GdbJ0a5XvdltTLUjL9v4z8GDrgtbARfLSey2mvXMI4fp7fz6aM3x31tqbHSJ+l03ThX6tzIYc/J1A7nmx9veLtPdZ1/EffwT/XGkZAb3m7xrftkuHjf4f8YNmvTKBvlj+uHjm8czjne8kd9L2NGDvbczG/C4LrW5IA14JrGqV0ffeiY7f8h7xbZqOeZr1+vOW7BwSDz9TNJQt3X8U8LomtujsvxxfT4PMaKWF3CgX+TZwcaj2+4Dq0bEsyLdeNzCX84FnE9+FpuFOK6kN2wZeeAl/t4y03FpDneNP/jp1beN6kb08b9Rtmvwik2x0C2VwkK7fCJ+H3xuHIR1uIyKWrka3sU4/flxYvw+1/m2Q/0fd8sGe/YHD+zdmSbJhK7oWG2T0fbjrPFL/+KyOjTrDkuO/F+p7rLLjvSq/VnxZ9bZYMdT17r2Bw/OQAjoV+HHLTtHrGYFahEti7Y5aPgaeF6EF+1OLePJ4sN/SsS2ceRif0Pso08vDPWT+TZS8Uy28H7xhx2ORx3Jp1fq+bmzlxzeim78L/g3+tr3hjjWL8eyJqyqMmQ3/3rSh375a5vbci9vj63cWzCr1tkv1Rt+PXrzOaYycudOHKAHCAHyAFygBwgB8iBygE2x0wGJgM5QA6QA+QAOUAOkAPkQOUAm2MmA5OBHCAHyAFygBwgB8gBcqBygM0xk4HJQA6QA+QAOUAOkAPkADlQOfAFzfHmy//65fHFl8uXZN3IXs7tXzJ/mS+C2xfpk/9OfRkbvxumtJfFjhwgB8gBcoAcIAcu4MD1zbE1cpP/WuzN3KaB/dLmOPyX6NCElseQ9P8iDU27PnfQ/pN/fNSJPqKl/cd/mHskKIbpYNcLNvJH/OEYFiNygBwgB8gBcoAc+GYcuLo5Lo/skF/aGZvE3hhLU/c6zbF/zEhphPuDrEc79bEl1qhq49p9LY0wNMDhgd06d3vjwKbXc4V4EA9ygBwgB8gBcoAceC4HrmyOeyPpG05zJu7A4k9Vhp/R1J1WaDLjz99ac9ruOjay2zizZX50zW/yU4vaAFf9fqzILHaU5wEWPHqjbT9P3JvpHdFVft11dnJUT/3JTv053vLTn/75hUV/2/GOTbntSLddbfypyxiP0eZyI2A75iGWF+C9w4DX51wlNsSGHCAHyAFygBz4Gg5c1xzjVyGGB6nHRrE30hrUsAMbfx/cN6G5rN48BtkXNmpe1y/9rfv2++u1ocTmtz8Yu+iVZrTYIo0yNpW9ge9zjgVUbOr+yZwqS5r0+rUOkYnjfp/ez6c/Jn+DmcoAW2M8wg54jA8T03DmkVwgB8gBcoAcIAd+Igeuao5xR7U0b9BsTZplaxKxqVNAsdGuu6Rv2OSiPHytY25ojrUJBLtNp56XXVK81vVoQ12b4o6DNce9aY4N7FHyDPgoJtUW8H8c1xO025XtYJdm2+IhY30z3n3t8eFu8dH4cVznIbEgFuQAOUAOkAPfkQNXNMeheao/N9gaLGjgCiB+/NDUYXNcd2vb1wPsawD2NYGN7MMBqHqsQbR52viirrYz3Jve5qfzO9sp9n6bjt1xwOdIc5zh1r6OUmxrdmvz37/Goj4bznB02Oic+rUKw8duJnjkP1qQA+QAOUAOkAPkwA/iwOXNcdaISVNlTdOmgR2aP5VXmzV8nYG8kb1rPPV6tb81i6ZHz+NusX0Xt9imTWRrOOVOEJvf8trLtN3ky+4aB3y2zXFofn/9Orud4/gd7k+/CzzqW9urOFisDTseWRTJAXKAHCAHyAFy4Idw4PLmeGhQw0f3ocnUZgoaMm3cWnNlO662k1l3aF0TCs3aRva2OZ41xhLMeg13TF0jWHdP23V9b3bXhhS+iuH9BB82xBmbVWiyAfs+rmDY7Kp+fBqGMCfFR/0INwULG1O/VMbnuelczE9t4HgWVHKAHCAHyAFygBx4EQ5c3Bz3pgwbPr9zqg1U/YhedlNlTmve3E6mNGXS3PUm03Zk8asVuCO7lo02ja/FDpRbXkNjaE2efb2gNfFVlruONpfraFvbST8UaLtJ8PYVv3fNsTXmNrc+2cKaY4e3jcF42A55v9b+KdF2oQ0PPY5+2z8KsjkeOcebAWJCDpAD5AA5QA58Lw5c3BwzwN8rwNq4pk1+0uQeauS/mf/0iTsR5AA5QA6QA+QAOXABB9gcXwDWd7wxyL4GoTvosWH+4Th8x9jRZt6IkQPkADlADpADX88BNsc/vims3+PGr0awMeYd9I/n/dcXUy5gxJwcIAfIgZ/BATbHbBLYKJID5AA5QA6QA+QAOUAOVA6wOWYyMBnIAXKAHCAHyAFygBwgByoH2BwzGZgM5AA5QA6QA+QAOUAOkAOVA2yOmQxMBnKAHCAHyAFygBwgB8iBygE2x0wGJgM5QA6QA+QAOUAOkAPkQOUAm2MmA5OBHCAHyAFygBwgB8gBcqBygM0xk4HJQA6QA+QAOUAOkAPkADlQOcDmmMnAZCAHyAFygBwgB8gBcoAcqBxgc8xkYDKQA+QAOUAOkAPkADlADlQOsDlmMjAZyAFygBwgB8gBcoAcIAcqB9gcMxmYDOQAOUAOkAPkADlADpADlQNsjpkMTAZygBwgB8gBcoAcIAfIgcoBNsdMBiYDOUAOkAPkADlADpAD5EDlAJtjJgOTgRwgB8gBcoAcIAfIAXKgcuD/bp2W05P6MSkAAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q58MJbckLUc"
      },
      "source": [
        "**Note: You only need to do this once per dataset. If you have already run this and are returning to conntinue training, skip these cells.**\n",
        "\n",
        "Next we need to convert our image dataset to a format that StyleGAN2-ADA can read from. There are two options here. You can upload your dataset directly to Colab (as a zipped file), or you can upload it to Drive directly and read it from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0QH0nzjlbEE"
      },
      "source": [
        "Now that your image dataset is uploaded, we need to convert it to the `.tfrecords` format.\n",
        "\n",
        "Depending on the resolution of your images and how many you have, this can take a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-BZHhBe7AvO",
        "outputId": "4d205de7-919e-48cb-8e07-94c6c63d80c8"
      },
      "source": [
        "%cd /content/drive/MyDrive/colab-sg2-ada/stylegan2-ada\n",
        "#update this to the path to your image folder\n",
        "dataset_path = \"path_of_dataset\"\n",
        "#give your dataset a name\n",
        "dataset_name = \"name\"\n",
        "!rm /content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/dataset/*\n",
        "#you don't need to edit anything here\n",
        "!python dataset_tool.py create_from_images ./datasets/{dataset_name} {dataset_path}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada\n",
            "Loading images from \"/content/drive/MyDrive/Dataset/landscape_final\"\n",
            "Creating dataset \"./datasets/landscape\"\n",
            "dataset_tool.py:97: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[quant.tostring()]))}))\n",
            "Added 2800 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DvTupHzP2s_"
      },
      "source": [
        "## Train a custom model\n",
        "\n",
        "We’re ready to start training! There are numerous arguments to training, what’s listed below are the most popular options. To see all the options, run the following cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxu7CA0Qb1Yd",
        "outputId": "95bfa11b-4a0e-47fb-975c-dd0dbea90baf"
      },
      "source": [
        "# Use them only for metrics and help with the arguments\n",
        "# This is just some help arguments\n",
        "!python train.py --help"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: train.py [-h] --outdir DIR [--gpus INT] [--snap INT] [--seed INT] [-n]\n",
            "                --data PATH [--res INT] [--mirror BOOL] [--mirrory BOOL]\n",
            "                [--use-raw BOOL] [--metrics LIST] [--metricdata PATH]\n",
            "                [--cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}]\n",
            "                [--lrate FLOAT] [--ttur BOOL] [--gamma FLOAT] [--nkimg INT]\n",
            "                [--kimg INT] [--topk FLOAT] [--aug {noaug,ada,fixed,adarv}]\n",
            "                [--p FLOAT] [--target TARGET] [--initstrength INITSTRENGTH]\n",
            "                [--augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}]\n",
            "                [--cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}]\n",
            "                [--dcap FLOAT] [--resume RESUME] [--freezed INT]\n",
            "\n",
            "Train a GAN using the techniques described in the paper\n",
            "\"Training Generative Adversarial Networks with Limited Data\".\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "general options:\n",
            "  --outdir DIR          Where to save the results (required)\n",
            "  --gpus INT            Number of GPUs to use (default: 1 gpu)\n",
            "  --snap INT            Snapshot interval (default: 50 ticks)\n",
            "  --seed INT            Random seed (default: 1000)\n",
            "  -n, --dry-run         Print training options and exit\n",
            "\n",
            "training dataset:\n",
            "  --data PATH           Training dataset path (required)\n",
            "  --res INT             Dataset resolution (default: highest available)\n",
            "  --mirror BOOL         Augment dataset with x-flips (default: false)\n",
            "  --mirrory BOOL        Augment dataset with y-flips (default: false)\n",
            "  --use-raw BOOL        Use raw image dataset, i.e. created from\n",
            "                        create_from_images_raw (default: False)\n",
            "\n",
            "metrics:\n",
            "  --metrics LIST        Comma-separated list or \"none\" (default: fid50k_full)\n",
            "  --metricdata PATH     Dataset to evaluate metrics against (optional)\n",
            "\n",
            "base config:\n",
            "  --cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}\n",
            "                        Base config (default: auto)\n",
            "  --lrate FLOAT         Override learning rate\n",
            "  --ttur BOOL           Use Two Time-Scale Update Rule (double learning rate\n",
            "                        for discriminator) (default: false)\n",
            "  --gamma FLOAT         Override R1 gamma\n",
            "  --nkimg INT           Override starting count\n",
            "  --kimg INT            Override training duration\n",
            "  --topk FLOAT          utilize top-k training\n",
            "\n",
            "discriminator augmentation:\n",
            "  --aug {noaug,ada,fixed,adarv}\n",
            "                        Augmentation mode (default: ada)\n",
            "  --p FLOAT             Specify augmentation probability for --aug=fixed\n",
            "  --target TARGET       Override ADA target for --aug=ada and --aug=adarv\n",
            "  --initstrength INITSTRENGTH\n",
            "                        Override ADA strength at start\n",
            "  --augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}\n",
            "                        Augmentation pipeline (default: bgc)\n",
            "\n",
            "comparison methods:\n",
            "  --cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}\n",
            "                        Comparison method (default: nocmethod)\n",
            "  --dcap FLOAT          Multiplier for discriminator capacity\n",
            "\n",
            "transfer learning:\n",
            "  --resume RESUME       Resume from network pickle (default: noresume)\n",
            "  --freezed INT         Freeze-D (default: 0 discriminator layers)\n",
            "\n",
            "examples:\n",
            "\n",
            "  # Train custom dataset using 1 GPU.\n",
            "  python train.py --outdir=~/training-runs --gpus=1 --data=~/datasets/custom\n",
            "\n",
            "  # Train class-conditional CIFAR-10 using 2 GPUs.\n",
            "  python train.py --outdir=~/training-runs --gpus=2 --data=~/datasets/cifar10c \\\n",
            "      --cfg=cifar\n",
            "\n",
            "  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n",
            "  python train.py --outdir=~/training-runs --gpus=4 --data=~/datasets/metfaces \\\n",
            "      --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n",
            "\n",
            "  # Reproduce original StyleGAN2 config F.\n",
            "  python train.py --outdir=~/training-runs --gpus=8 --data=~/datasets/ffhq \\\n",
            "      --cfg=stylegan2 --res=1024 --mirror=1 --aug=noaug\n",
            "\n",
            "available base configs (--cfg):\n",
            "  auto           Automatically select reasonable defaults based on resolution\n",
            "                 and GPU count. Good starting point for new datasets.\n",
            "  stylegan2      Reproduce results for StyleGAN2 config F at 1024x1024.\n",
            "  paper256       Reproduce results for FFHQ and LSUN Cat at 256x256.\n",
            "  paper512       Reproduce results for BreCaHAD and AFHQ at 512x512.\n",
            "  paper1024      Reproduce results for MetFaces at 1024x1024.\n",
            "  cifar          Reproduce results for CIFAR-10 (tuned configuration).\n",
            "  cifarbaseline  Reproduce results for CIFAR-10 (baseline configuration).\n",
            "\n",
            "transfer learning source networks (--resume):\n",
            "  ffhq256        FFHQ trained at 256x256 resolution.\n",
            "  ffhq512        FFHQ trained at 512x512 resolution.\n",
            "  ffhq1024       FFHQ trained at 1024x1024 resolution.\n",
            "  celebahq256    CelebA-HQ trained at 256x256 resolution.\n",
            "  lsundog256     LSUN Dog trained at 256x256 resolution.\n",
            "  afhqcat512     AFHQ Cat trained at 512x512 resolution.\n",
            "  afhqdog512     AFHQ Dog trained at 512x512 resolution.\n",
            "  afhqwild512    AFHQ Wild trained at 512x512 resolution.\n",
            "  brecahad512    BreCaHAD trained at 512x512 resolution.\n",
            "  cifar10        CIFAR10 trained at 32x32 resolution.\n",
            "  metfaces512    MetFaces trained at 512x512 resolution.\n",
            "  <path or URL>  Custom network pickle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD-IOz_6jZHX"
      },
      "source": [
        "## Arguments are down below\n",
        "\n",
        "The name of the output directory (e.g., `00000-custom-auto1`) reflects the hyperparameter configuration that was used. In this case, `custom` indicates the training set (`--data`) and `auto1` indicates the *base configuration* that was used to select the hyperparameters (`--cfg`):\n",
        "\n",
        "| Base config      | Description\n",
        "| :----------      | :----------\n",
        "| `auto` (default) | Automatically select reasonable defaults based on resolution and GPU count. Serves as a good starting point for new datasets, but does not necessarily lead to optimal results.\n",
        "| `stylegan2`      | Reproduce results for StyleGAN2 config F at 1024x1024 using 1, 2, 4, or 8 GPUs.\n",
        "| `paper256`       | Reproduce results for FFHQ and LSUN Cat at 256x256 using 1, 2, 4, or 8 GPUs.\n",
        "| `paper512`       | Reproduce results for BreCaHAD and AFHQ at 512x512 using 1, 2, 4, or 8 GPUs.\n",
        "| `paper1024`      | Reproduce results for MetFaces at 1024x1024 using 1, 2, 4, or 8 GPUs.\n",
        "| `cifar`          | Reproduce results for CIFAR-10 (tuned configuration) using 1 or 2 GPUs.\n",
        "| `cifarbaseline`  | Reproduce results for CIFAR-10 (baseline configuration) using 1 or 2 GPUs.\n",
        "\n",
        "The training configuration can be further customized with additional arguments. Common examples:\n",
        "\n",
        "* `--aug=noaug` disables ADA (default: enabled).\n",
        "* `--mirror=1` amplifies the dataset with x-flips. Often beneficial, even with ADA.\n",
        "* `--resume=ffhq1024 --snap=10` performs transfer learning from FFHQ trained at 1024x1024.\n",
        "* `--resume=~/training-runs/<RUN_NAME>/network-snapshot-<KIMG>.pkl` resumes where a previous training run left off.\n",
        "* `--gamma=10` overrides R1 gamma. We strongly recommend trying out at least a few different values for each new dataset.\n",
        "\n",
        "Augmentation fine-tuning:\n",
        "\n",
        "* `--aug=ada --target=0.7` adjusts ADA target value (default: 0.6).\n",
        "* `--aug=adarv` selects the alternative ADA heuristic (requires a separate validation set).\n",
        "* `--augpipe=blit` limits the augmentation pipeline to pixel blitting only.\n",
        "* `--augpipe=bgcfnc` enables all available augmentations (blit, geom, color, filter, noise, cutout).\n",
        "* `--cmethod=bcr` enables bCR with small integer translations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk7C93HzvrWx"
      },
      "source": [
        "dataset_name = \"\" #@param[]{allow-input: true}\n",
        "#how often should the model generate samples and a .pkl file\n",
        "snapshot_count = 4 #@param {type: \"number\"}\n",
        "#should the images be mirrored left to right?\n",
        "mirrored_X = 0 #@param {type: \"slider\", min: 0, max: 1}\n",
        "#should the images be mirrored top to bottom?\n",
        "mirrored_Y = 0 #@param {type: \"slider\", min: 0, max: 1}\n",
        "\n",
        "#metrics? \n",
        "metric_list = None\n",
        "#augments\n",
        "\n",
        "aug = 'ada'\n",
        "\n",
        "# target min: 0.5, max: 1.0\n",
        "target = 0.6 #@param {type: \"number\"}\n",
        "augpipe= 'bgcnfnc'  #@param [\"blit\", \"bgcnfnc\", \"bgc\"]\n",
        "#\n",
        "# this is the most important cell to update\n",
        "#\n",
        "# Change this after each training session\n",
        "resume_from_path = \"Latest_pkl_file_in_results_folder\" #@param[]{allow-input: true}\n",
        "\n",
        "#don't edit this unless you know what you're doing :)\n",
        "!python train.py --outdir ./results --snap={snapshot_count} --aug={aug} --target={target} --cfg=11gb-gpu --gpus=1 --augpipe={augpipe} --data=./datasets/{dataset_name}  --mirror={mirrored_X} --mirrory={mirrored_Y} --metrics={metric_list} --resume={resume_from_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOftFoyiDU3s",
        "outputId": "1f4b1f34-e05a-4f73-ce72-d038f7fa6a7a"
      },
      "source": [
        "#this name must EXACTLY match the dataset name you used when creating the .tfrecords file\n",
        "dataset_name = \"dima-art\"\n",
        "#how often should the model generate samples and a .pkl file\n",
        "snapshot_count = 4\n",
        "#should the images be mirrored left to right?\n",
        "mirrored = True\n",
        "#should the images be mirrored top to bottom?\n",
        "mirroredY = False\n",
        "#metrics? \n",
        "metric_list = None\n",
        "#augments\n",
        "augs = \"bg\"\n",
        "\n",
        "#\n",
        "# this is the most important cell to update\n",
        "#\n",
        "# running it for the first time? set it to ffhq(+resolution)\n",
        "# resuming? get the path to your latest .pkl file and use that\n",
        "resume_from = \"/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00008-dima-art-mirror-11gb-gpu-bg-resumecustom/network-snapshot-000016.pkl\" # Change this after each training session\n",
        "\n",
        "#don't edit this unless you know what you're doing :)\n",
        "!python train.py --outdir ./results --snap={snapshot_count} --cfg=11gb-gpu --data=./datasets/{dataset_name} --augpipe={augs} --mirror={mirrored} --mirrory={mirroredY} --metrics={metric_list} --resume={resume_from} --augpipe=\"bg\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 4294967296 bytes == 0x55d4f19b0000 @  0x7fae4213e001 0x7fae3f34154f 0x7fae3f391b58 0x7fae3f395b17 0x7fae3f434203 0x55d4e8f3c544 0x55d4e8f3c240 0x55d4e8fb0627 0x55d4e8faaced 0x55d4e8f3e48c 0x55d4e8f7f159 0x55d4e8f7c0a4 0x55d4e8f3cd49 0x55d4e8fb094f 0x55d4e8faa9ee 0x55d4e8e7ce2b 0x55d4e8facfe4 0x55d4e8faa9ee 0x55d4e8e7ce2b 0x55d4e8facfe4 0x55d4e8faaced 0x55d4e8e7ce2b 0x55d4e8facfe4 0x55d4e8f3dafa 0x55d4e8fab915 0x55d4e8faa9ee 0x55d4e8faa6f3 0x55d4e90744c2 0x55d4e907483d 0x55d4e90746e6 0x55d4e904c163\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55d5f19b0000 @  0x7fae4213c1e7 0x7fae3f34146e 0x7fae3f391c7b 0x7fae3f39235f 0x7fae3f434103 0x55d4e8f3c544 0x55d4e8f3c240 0x55d4e8fb0627 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fac737 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fac737 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fac737 0x55d4e8f3dafa 0x55d4e8fab915 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fafd00 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fac737 0x55d4e8faaced 0x55d4e8f3e48c 0x55d4e8f7f159 0x55d4e8f7c0a4 0x55d4e8f3cd49 0x55d4e8fb094f\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55d6f2c74000 @  0x7fae4213c1e7 0x7fae3f34146e 0x7fae3f391c7b 0x7fae3f39235f 0x7fadead41235 0x7fadea6c4792 0x7fadea6c4d42 0x7fadea67daee 0x55d4e8f3c437 0x55d4e8f3c240 0x55d4e8fb00f3 0x55d4e8f3dafa 0x55d4e8fabc0d 0x55d4e8faaced 0x55d4e8e7ceb0 0x55d4e8facfe4 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fabc0d 0x55d4e8faaced 0x55d4e8f3dbda 0x55d4e8fabc0d 0x55d4e8f3dafa 0x55d4e8fabc0d 0x55d4e8faa9ee 0x55d4e8f3e271 0x55d4e8f3e698 0x55d4e8facfe4 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fab915\n",
            "\n",
            "Training options:\n",
            "{\n",
            "  \"G_args\": {\n",
            "    \"func_name\": \"training.networks.G_main\",\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"mapping_layers\": 8,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"D_args\": {\n",
            "    \"func_name\": \"training.networks.D_main\",\n",
            "    \"mbstd_group_size\": 4,\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.002\n",
            "  },\n",
            "  \"D_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.002\n",
            "  },\n",
            "  \"loss_args\": {\n",
            "    \"func_name\": \"training.loss.stylegan2\",\n",
            "    \"r1_gamma\": 10\n",
            "  },\n",
            "  \"augment_args\": {\n",
            "    \"class_name\": \"training.augment.AdaptiveAugment\",\n",
            "    \"tune_heuristic\": \"rt\",\n",
            "    \"tune_target\": 0.6,\n",
            "    \"apply_func\": \"training.augment.augment_pipeline\",\n",
            "    \"apply_args\": {\n",
            "      \"xflip\": 1,\n",
            "      \"rotate90\": 1,\n",
            "      \"xint\": 1,\n",
            "      \"scale\": 1,\n",
            "      \"rotate\": 1,\n",
            "      \"aniso\": 1,\n",
            "      \"xfrac\": 1\n",
            "    },\n",
            "    \"tune_kimg\": 100\n",
            "  },\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 4,\n",
            "  \"network_snapshot_ticks\": 4,\n",
            "  \"train_dataset_args\": {\n",
            "    \"path\": \"./datasets/dima-art\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 1024,\n",
            "    \"mirror_augment\": true,\n",
            "    \"mirror_augment_v\": false\n",
            "  },\n",
            "  \"metric_arg_list\": [],\n",
            "  \"metric_dataset_args\": {\n",
            "    \"path\": \"./datasets/dima-art\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 1024,\n",
            "    \"mirror_augment\": true,\n",
            "    \"mirror_augment_v\": false\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"minibatch_size\": 4,\n",
            "  \"minibatch_gpu\": 4,\n",
            "  \"G_smoothing_kimg\": 10,\n",
            "  \"G_smoothing_rampup\": null,\n",
            "  \"resume_pkl\": \"/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00008-dima-art-mirror-11gb-gpu-bg-resumecustom/network-snapshot-000016.pkl\",\n",
            "  \"run_dir\": \"./results/00009-dima-art-mirror-11gb-gpu-bg-resumecustom\"\n",
            "}\n",
            "\n",
            "Output directory:  ./results/00009-dima-art-mirror-11gb-gpu-bg-resumecustom\n",
            "Training data:     ./datasets/dima-art\n",
            "Training length:   25000 kimg\n",
            "Resolution:        1024\n",
            "Number of GPUs:    1\n",
            "\n",
            "Creating output directory...\n",
            "Loading training set...\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55d4f16ae000 @  0x7fae4213e001 0x7fae3f34154f 0x7fae3f391b58 0x7fae3f395b17 0x7fae3f434203 0x55d4e8f3c544 0x55d4e8f3c240 0x55d4e8fb0627 0x55d4e8faaced 0x55d4e8f3e48c 0x55d4e8f7f159 0x55d4e8f7c0a4 0x55d4e8f3cd49 0x55d4e8fb094f 0x55d4e8faa9ee 0x55d4e8e7ce2b 0x55d4e8facfe4 0x55d4e8faa9ee 0x55d4e8e7ce2b 0x55d4e8facfe4 0x55d4e8faaced 0x55d4e8e7ce2b 0x55d4e8facfe4 0x55d4e8f3dafa 0x55d4e8fab915 0x55d4e8faa9ee 0x55d4e8faa6f3 0x55d4e90744c2 0x55d4e907483d 0x55d4e90746e6 0x55d4e904c163\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55d7f2c74000 @  0x7fae4213c1e7 0x7fae3f34146e 0x7fae3f391c7b 0x7fae3f39235f 0x7fae3f434103 0x55d4e8f3c544 0x55d4e8f3c240 0x55d4e8fb0627 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fac737 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fac737 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fac737 0x55d4e8f3dafa 0x55d4e8fab915 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fafd00 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fac737 0x55d4e8faaced 0x55d4e8f3e48c 0x55d4e8f7f159 0x55d4e8f7c0a4 0x55d4e8f3cd49 0x55d4e8fb094f\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55d7f2c74000 @  0x7fae4213c1e7 0x7fae3f34146e 0x7fae3f391c7b 0x7fae3f39235f 0x7fadead41235 0x7fadea6c4792 0x7fadea6c4d42 0x7fadea67daee 0x55d4e8f3c437 0x55d4e8f3c240 0x55d4e8fb00f3 0x55d4e8f3dafa 0x55d4e8fabc0d 0x55d4e8faaced 0x55d4e8e7ceb0 0x55d4e8facfe4 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fabc0d 0x55d4e8faaced 0x55d4e8f3dbda 0x55d4e8fabc0d 0x55d4e8f3dafa 0x55d4e8fabc0d 0x55d4e8faa9ee 0x55d4e8f3e271 0x55d4e8f3e698 0x55d4e8facfe4 0x55d4e8faa9ee 0x55d4e8f3dbda 0x55d4e8fab915\n",
            "Image shape: [3, 1024, 1024]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
            "Resuming from \"/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00008-dima-art-mirror-11gb-gpu-bg-resumecustom/network-snapshot-000016.pkl\"\n",
            "\n",
            "G                               Params    OutputShape          WeightShape     \n",
            "---                             ---       ---                  ---             \n",
            "latents_in                      -         (?, 512)             -               \n",
            "labels_in                       -         (?, 0)               -               \n",
            "epochs                          1         ()                   ()              \n",
            "epochs_1                        1         ()                   ()              \n",
            "G_mapping/Normalize             -         (?, 512)             -               \n",
            "G_mapping/Dense0                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense1                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense2                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense3                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense4                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense5                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense6                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense7                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Broadcast             -         (?, 18, 512)         -               \n",
            "dlatent_avg                     -         (512,)               -               \n",
            "Truncation/Lerp                 -         (?, 18, 512)         -               \n",
            "G_synthesis/4x4/Const           8192      (?, 512, 4, 4)       (1, 512, 4, 4)  \n",
            "G_synthesis/4x4/Conv            2622465   (?, 512, 4, 4)       (3, 3, 512, 512)\n",
            "G_synthesis/4x4/ToRGB           264195    (?, 3, 4, 4)         (1, 1, 512, 3)  \n",
            "G_synthesis/8x8/Conv0_up        2622465   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Conv1           2622465   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Upsample        -         (?, 3, 8, 8)         -               \n",
            "G_synthesis/8x8/ToRGB           264195    (?, 3, 8, 8)         (1, 1, 512, 3)  \n",
            "G_synthesis/16x16/Conv0_up      2622465   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Conv1         2622465   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Upsample      -         (?, 3, 16, 16)       -               \n",
            "G_synthesis/16x16/ToRGB         264195    (?, 3, 16, 16)       (1, 1, 512, 3)  \n",
            "G_synthesis/32x32/Conv0_up      2622465   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Conv1         2622465   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Upsample      -         (?, 3, 32, 32)       -               \n",
            "G_synthesis/32x32/ToRGB         264195    (?, 3, 32, 32)       (1, 1, 512, 3)  \n",
            "G_synthesis/64x64/Conv0_up      2622465   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Conv1         2622465   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Upsample      -         (?, 3, 64, 64)       -               \n",
            "G_synthesis/64x64/ToRGB         264195    (?, 3, 64, 64)       (1, 1, 512, 3)  \n",
            "G_synthesis/128x128/Conv0_up    1442561   (?, 256, 128, 128)   (3, 3, 512, 256)\n",
            "G_synthesis/128x128/Conv1       721409    (?, 256, 128, 128)   (3, 3, 256, 256)\n",
            "G_synthesis/128x128/Upsample    -         (?, 3, 128, 128)     -               \n",
            "G_synthesis/128x128/ToRGB       132099    (?, 3, 128, 128)     (1, 1, 256, 3)  \n",
            "G_synthesis/256x256/Conv0_up    426369    (?, 128, 256, 256)   (3, 3, 256, 128)\n",
            "G_synthesis/256x256/Conv1       213249    (?, 128, 256, 256)   (3, 3, 128, 128)\n",
            "G_synthesis/256x256/Upsample    -         (?, 3, 256, 256)     -               \n",
            "G_synthesis/256x256/ToRGB       66051     (?, 3, 256, 256)     (1, 1, 128, 3)  \n",
            "G_synthesis/512x512/Conv0_up    139457    (?, 64, 512, 512)    (3, 3, 128, 64) \n",
            "G_synthesis/512x512/Conv1       69761     (?, 64, 512, 512)    (3, 3, 64, 64)  \n",
            "G_synthesis/512x512/Upsample    -         (?, 3, 512, 512)     -               \n",
            "G_synthesis/512x512/ToRGB       33027     (?, 3, 512, 512)     (1, 1, 64, 3)   \n",
            "G_synthesis/1024x1024/Conv0_up  51297     (?, 32, 1024, 1024)  (3, 3, 64, 32)  \n",
            "G_synthesis/1024x1024/Conv1     25665     (?, 32, 1024, 1024)  (3, 3, 32, 32)  \n",
            "G_synthesis/1024x1024/Upsample  -         (?, 3, 1024, 1024)   -               \n",
            "G_synthesis/1024x1024/ToRGB     16515     (?, 3, 1024, 1024)   (1, 1, 32, 3)   \n",
            "---                             ---       ---                  ---             \n",
            "Total                           30370062                                       \n",
            "\n",
            "\n",
            "D                     Params    OutputShape          WeightShape     \n",
            "---                   ---       ---                  ---             \n",
            "images_in             -         (?, 3, 1024, 1024)   -               \n",
            "labels_in             -         (?, 0)               -               \n",
            "1024x1024/FromRGB     128       (?, 32, 1024, 1024)  (1, 1, 3, 32)   \n",
            "1024x1024/Conv0       9248      (?, 32, 1024, 1024)  (3, 3, 32, 32)  \n",
            "1024x1024/Conv1_down  18496     (?, 64, 512, 512)    (3, 3, 32, 64)  \n",
            "1024x1024/Skip        2048      (?, 64, 512, 512)    (1, 1, 32, 64)  \n",
            "512x512/Conv0         36928     (?, 64, 512, 512)    (3, 3, 64, 64)  \n",
            "512x512/Conv1_down    73856     (?, 128, 256, 256)   (3, 3, 64, 128) \n",
            "512x512/Skip          8192      (?, 128, 256, 256)   (1, 1, 64, 128) \n",
            "256x256/Conv0         147584    (?, 128, 256, 256)   (3, 3, 128, 128)\n",
            "256x256/Conv1_down    295168    (?, 256, 128, 128)   (3, 3, 128, 256)\n",
            "256x256/Skip          32768     (?, 256, 128, 128)   (1, 1, 128, 256)\n",
            "128x128/Conv0         590080    (?, 256, 128, 128)   (3, 3, 256, 256)\n",
            "128x128/Conv1_down    1180160   (?, 512, 64, 64)     (3, 3, 256, 512)\n",
            "128x128/Skip          131072    (?, 512, 64, 64)     (1, 1, 256, 512)\n",
            "64x64/Conv0           2359808   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "64x64/Conv1_down      2359808   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "64x64/Skip            262144    (?, 512, 32, 32)     (1, 1, 512, 512)\n",
            "32x32/Conv0           2359808   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "32x32/Conv1_down      2359808   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "32x32/Skip            262144    (?, 512, 16, 16)     (1, 1, 512, 512)\n",
            "16x16/Conv0           2359808   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "16x16/Conv1_down      2359808   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "16x16/Skip            262144    (?, 512, 8, 8)       (1, 1, 512, 512)\n",
            "8x8/Conv0             2359808   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "8x8/Conv1_down        2359808   (?, 512, 4, 4)       (3, 3, 512, 512)\n",
            "8x8/Skip              262144    (?, 512, 4, 4)       (1, 1, 512, 512)\n",
            "4x4/MinibatchStddev   -         (?, 513, 4, 4)       -               \n",
            "4x4/Conv              2364416   (?, 512, 4, 4)       (3, 3, 513, 512)\n",
            "4x4/Dense0            4194816   (?, 512)             (8192, 512)     \n",
            "Output                513       (?, 1)               (512, 1)        \n",
            "---                   ---       ---                  ---             \n",
            "Total                 29012513                                       \n",
            "\n",
            "Exporting sample images...\n",
            "Replicating networks across 1 GPUs...\n",
            "Initializing augmentations...\n",
            "Setting up optimizers...\n",
            "Constructing training graph...\n",
            "Finalizing training ops...\n",
            "Initializing metrics...\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 3m 17s       sec/tick 80.9    sec/kimg 5059.15 maintenance 116.2  gpumem 10.3  augment 0.000\n",
            "tick 1     kimg 4.0      time 35m 37s      sec/tick 1922.4  sec/kimg 480.61  maintenance 17.5   gpumem 10.3  augment 0.035\n",
            "tick 2     kimg 8.0      time 1h 07m 49s   sec/tick 1931.8  sec/kimg 482.94  maintenance 0.0    gpumem 10.3  augment 0.071\n",
            "tick 3     kimg 12.0     time 1h 40m 09s   sec/tick 1939.8  sec/kimg 484.95  maintenance 0.0    gpumem 10.3  augment 0.104\n",
            "tick 4     kimg 16.0     time 2h 12m 37s   sec/tick 1948.7  sec/kimg 487.19  maintenance 0.0    gpumem 10.3  augment 0.138\n",
            "tick 5     kimg 20.0     time 2h 45m 17s   sec/tick 1954.6  sec/kimg 488.66  maintenance 4.5    gpumem 10.3  augment 0.170\n",
            "tick 6     kimg 24.0     time 3h 17m 59s   sec/tick 1962.4  sec/kimg 490.59  maintenance 0.0    gpumem 10.3  augment 0.206\n",
            "tick 7     kimg 28.0     time 3h 50m 46s   sec/tick 1967.2  sec/kimg 491.81  maintenance 0.0    gpumem 10.3  augment 0.236\n",
            "tick 8     kimg 32.0     time 4h 23m 40s   sec/tick 1973.5  sec/kimg 493.38  maintenance 0.0    gpumem 10.3  augment 0.270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lohotw1FqC54"
      },
      "source": [
        "### While it’s training...\n",
        "**Once the above cell is running you should be training!**\n",
        "\n",
        "Don’t close this tab! Colab needs to be open and running in order to continue training. Every ~15min or so a new line should get added to your output, indicated its still training. Depending on you `snapshot_count` setting you should see the results folder in your Google drive folder fill with both samples (`fakesXXXXXx.jpg`) and model weights (`network-snapshot-XXXXXX.pkl`). The samples are worth looking at while it trains but don’t get too worried about each individual sample.\n",
        "\n",
        "If you chose a metric, you will also see scores for each snapshot. Don’t obsess over these! they are a guide, it can go up or down slightly for each snapshot. What you want to see is a gradual lowering of the score over time.\n",
        "\n",
        "Once Colab shuts off, you can Reconnect the notebook and re-run every cell from top to bottom. Make sure you update the `resume_from` path to continue training from the latest model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUdqO1Djaryp"
      },
      "source": [
        "## Training model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtQTrX3Bapca",
        "outputId": "48006dad-4797-40fb-8833-d15bb459a4c7"
      },
      "source": [
        "dataset_name = \"\" #@param[]{allow-input: true}\n",
        "#how often should the model generate samples and a .pkl file\n",
        "snapshot_count = 4 #@param {type: \"number\"}\n",
        "#should the images be mirrored left to right?\n",
        "mirrored_X = 0 #@param {type: \"slider\", min: 0, max: 1}\n",
        "#should the images be mirrored top to bottom?\n",
        "mirrored_Y = 0 #@param {type: \"slider\", min: 0, max: 1}\n",
        "\n",
        "#metrics? \n",
        "metric_list = None\n",
        "#augments\n",
        "\n",
        "aug = 'ada'\n",
        "\n",
        "# target min: 0.5, max: 1.0\n",
        "target = 0.6 #@param {type: \"number\"}\n",
        "augpipe= 'bgcnfnc'  #@param [\"blit\", \"bgcnfnc\", \"bgc\"]\n",
        "\n",
        "\n",
        "!python train.py --outdir ./results --snap={snapshot_count} --aug={aug} --target={target} --augpipe={augpipe} --cfg=11gb-gpu --data=./datasets/{dataset_name} --mirror={mirrored_X} --mirrory={mirrored_Y} --metrics={metric_list}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 4294967296 bytes == 0x559d90e62000 @  0x7f1461ba5001 0x7f145eda854f 0x7f145edf8b58 0x7f145edfcb17 0x7f145ee9b203 0x559d89d5d544 0x559d89d5d240 0x559d89dd1627 0x559d89dcbced 0x559d89d5f48c 0x559d89da0159 0x559d89d9d0a4 0x559d89d5dd49 0x559d89dd194f 0x559d89dcb9ee 0x559d89c9de2b 0x559d89dcdfe4 0x559d89dcb9ee 0x559d89c9de2b 0x559d89dcdfe4 0x559d89dcbced 0x559d89c9de2b 0x559d89dcdfe4 0x559d89d5eafa 0x559d89dcc915 0x559d89dcb9ee 0x559d89dcb6f3 0x559d89e954c2 0x559d89e9583d 0x559d89e956e6 0x559d89e6d163\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x559e90e62000 @  0x7f1461ba31e7 0x7f145eda846e 0x7f145edf8c7b 0x7f145edf935f 0x7f145ee9b103 0x559d89d5d544 0x559d89d5d240 0x559d89dd1627 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dcd737 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dcd737 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dcd737 0x559d89d5eafa 0x559d89dcc915 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dd0d00 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dcd737 0x559d89dcbced 0x559d89d5f48c 0x559d89da0159 0x559d89d9d0a4 0x559d89d5dd49 0x559d89dd194f\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x559f91fa6000 @  0x7f1461ba31e7 0x7f145eda846e 0x7f145edf8c7b 0x7f145edf935f 0x7f141b7aa235 0x7f141b12d792 0x7f141b12dd42 0x7f141b0e6aee 0x559d89d5d437 0x559d89d5d240 0x559d89dd10f3 0x559d89d5eafa 0x559d89dccc0d 0x559d89dcbced 0x559d89c9deb0 0x559d89dcdfe4 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dccc0d 0x559d89dcbced 0x559d89d5ebda 0x559d89dccc0d 0x559d89d5eafa 0x559d89dccc0d 0x559d89dcb9ee 0x559d89d5f271 0x559d89d5f698 0x559d89dcdfe4 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dcc915\n",
            "\n",
            "Training options:\n",
            "{\n",
            "  \"G_args\": {\n",
            "    \"func_name\": \"training.networks.G_main\",\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"mapping_layers\": 2,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"D_args\": {\n",
            "    \"func_name\": \"training.networks.D_main\",\n",
            "    \"mbstd_group_size\": 4,\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.002\n",
            "  },\n",
            "  \"D_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.002\n",
            "  },\n",
            "  \"loss_args\": {\n",
            "    \"func_name\": \"training.loss.stylegan2\",\n",
            "    \"r1_gamma\": 52.4288\n",
            "  },\n",
            "  \"augment_args\": {\n",
            "    \"class_name\": \"training.augment.AdaptiveAugment\",\n",
            "    \"tune_heuristic\": \"rt\",\n",
            "    \"tune_target\": 0.6,\n",
            "    \"apply_func\": \"training.augment.augment_pipeline\",\n",
            "    \"apply_args\": {\n",
            "      \"xflip\": 1,\n",
            "      \"rotate90\": 1,\n",
            "      \"xint\": 1,\n",
            "      \"scale\": 1,\n",
            "      \"rotate\": 1,\n",
            "      \"aniso\": 1,\n",
            "      \"xfrac\": 1\n",
            "    }\n",
            "  },\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 4,\n",
            "  \"network_snapshot_ticks\": 4,\n",
            "  \"train_dataset_args\": {\n",
            "    \"path\": \"./datasets/robgon-abstract\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 1024,\n",
            "    \"mirror_augment\": true,\n",
            "    \"mirror_augment_v\": true\n",
            "  },\n",
            "  \"metric_arg_list\": [],\n",
            "  \"metric_dataset_args\": {\n",
            "    \"path\": \"./datasets/robgon-abstract\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 1024,\n",
            "    \"mirror_augment\": true,\n",
            "    \"mirror_augment_v\": true\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"minibatch_size\": 4,\n",
            "  \"minibatch_gpu\": 4,\n",
            "  \"G_smoothing_kimg\": 1.25,\n",
            "  \"G_smoothing_rampup\": 0.05,\n",
            "  \"run_dir\": \"./results/00000-robgon-abstract-mirror-mirrory-auto1-bg\"\n",
            "}\n",
            "\n",
            "Output directory:  ./results/00000-robgon-abstract-mirror-mirrory-auto1-bg\n",
            "Training data:     ./datasets/robgon-abstract\n",
            "Training length:   25000 kimg\n",
            "Resolution:        1024\n",
            "Number of GPUs:    1\n",
            "\n",
            "Creating output directory...\n",
            "Loading training set...\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x559d90b60000 @  0x7f1461ba5001 0x7f145eda854f 0x7f145edf8b58 0x7f145edfcb17 0x7f145ee9b203 0x559d89d5d544 0x559d89d5d240 0x559d89dd1627 0x559d89dcbced 0x559d89d5f48c 0x559d89da0159 0x559d89d9d0a4 0x559d89d5dd49 0x559d89dd194f 0x559d89dcb9ee 0x559d89c9de2b 0x559d89dcdfe4 0x559d89dcb9ee 0x559d89c9de2b 0x559d89dcdfe4 0x559d89dcbced 0x559d89c9de2b 0x559d89dcdfe4 0x559d89d5eafa 0x559d89dcc915 0x559d89dcb9ee 0x559d89dcb6f3 0x559d89e954c2 0x559d89e9583d 0x559d89e956e6 0x559d89e6d163\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55a091fa6000 @  0x7f1461ba31e7 0x7f145eda846e 0x7f145edf8c7b 0x7f145edf935f 0x7f145ee9b103 0x559d89d5d544 0x559d89d5d240 0x559d89dd1627 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dcd737 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dcd737 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dcd737 0x559d89d5eafa 0x559d89dcc915 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dd0d00 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dcd737 0x559d89dcbced 0x559d89d5f48c 0x559d89da0159 0x559d89d9d0a4 0x559d89d5dd49 0x559d89dd194f\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55a091fa6000 @  0x7f1461ba31e7 0x7f145eda846e 0x7f145edf8c7b 0x7f145edf935f 0x7f141b7aa235 0x7f141b12d792 0x7f141b12dd42 0x7f141b0e6aee 0x559d89d5d437 0x559d89d5d240 0x559d89dd10f3 0x559d89d5eafa 0x559d89dccc0d 0x559d89dcbced 0x559d89c9deb0 0x559d89dcdfe4 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dccc0d 0x559d89dcbced 0x559d89d5ebda 0x559d89dccc0d 0x559d89d5eafa 0x559d89dccc0d 0x559d89dcb9ee 0x559d89d5f271 0x559d89d5f698 0x559d89dcdfe4 0x559d89dcb9ee 0x559d89d5ebda 0x559d89dcc915\n",
            "Image shape: [3, 1024, 1024]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Compiling... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Compiling... Loading... Done.\n",
            "\n",
            "G                               Params    OutputShape          WeightShape     \n",
            "---                             ---       ---                  ---             \n",
            "latents_in                      -         (?, 512)             -               \n",
            "labels_in                       -         (?, 0)               -               \n",
            "epochs                          1         ()                   ()              \n",
            "epochs_1                        1         ()                   ()              \n",
            "G_mapping/Normalize             -         (?, 512)             -               \n",
            "G_mapping/Dense0                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense1                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Broadcast             -         (?, 18, 512)         -               \n",
            "dlatent_avg                     -         (512,)               -               \n",
            "Truncation/Lerp                 -         (?, 18, 512)         -               \n",
            "G_synthesis/4x4/Const           8192      (?, 512, 4, 4)       (1, 512, 4, 4)  \n",
            "G_synthesis/4x4/Conv            2622465   (?, 512, 4, 4)       (3, 3, 512, 512)\n",
            "G_synthesis/4x4/ToRGB           264195    (?, 3, 4, 4)         (1, 1, 512, 3)  \n",
            "G_synthesis/8x8/Conv0_up        2622465   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Conv1           2622465   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Upsample        -         (?, 3, 8, 8)         -               \n",
            "G_synthesis/8x8/ToRGB           264195    (?, 3, 8, 8)         (1, 1, 512, 3)  \n",
            "G_synthesis/16x16/Conv0_up      2622465   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Conv1         2622465   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Upsample      -         (?, 3, 16, 16)       -               \n",
            "G_synthesis/16x16/ToRGB         264195    (?, 3, 16, 16)       (1, 1, 512, 3)  \n",
            "G_synthesis/32x32/Conv0_up      2622465   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Conv1         2622465   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Upsample      -         (?, 3, 32, 32)       -               \n",
            "G_synthesis/32x32/ToRGB         264195    (?, 3, 32, 32)       (1, 1, 512, 3)  \n",
            "G_synthesis/64x64/Conv0_up      2622465   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Conv1         2622465   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Upsample      -         (?, 3, 64, 64)       -               \n",
            "G_synthesis/64x64/ToRGB         264195    (?, 3, 64, 64)       (1, 1, 512, 3)  \n",
            "G_synthesis/128x128/Conv0_up    1442561   (?, 256, 128, 128)   (3, 3, 512, 256)\n",
            "G_synthesis/128x128/Conv1       721409    (?, 256, 128, 128)   (3, 3, 256, 256)\n",
            "G_synthesis/128x128/Upsample    -         (?, 3, 128, 128)     -               \n",
            "G_synthesis/128x128/ToRGB       132099    (?, 3, 128, 128)     (1, 1, 256, 3)  \n",
            "G_synthesis/256x256/Conv0_up    426369    (?, 128, 256, 256)   (3, 3, 256, 128)\n",
            "G_synthesis/256x256/Conv1       213249    (?, 128, 256, 256)   (3, 3, 128, 128)\n",
            "G_synthesis/256x256/Upsample    -         (?, 3, 256, 256)     -               \n",
            "G_synthesis/256x256/ToRGB       66051     (?, 3, 256, 256)     (1, 1, 128, 3)  \n",
            "G_synthesis/512x512/Conv0_up    139457    (?, 64, 512, 512)    (3, 3, 128, 64) \n",
            "G_synthesis/512x512/Conv1       69761     (?, 64, 512, 512)    (3, 3, 64, 64)  \n",
            "G_synthesis/512x512/Upsample    -         (?, 3, 512, 512)     -               \n",
            "G_synthesis/512x512/ToRGB       33027     (?, 3, 512, 512)     (1, 1, 64, 3)   \n",
            "G_synthesis/1024x1024/Conv0_up  51297     (?, 32, 1024, 1024)  (3, 3, 64, 32)  \n",
            "G_synthesis/1024x1024/Conv1     25665     (?, 32, 1024, 1024)  (3, 3, 32, 32)  \n",
            "G_synthesis/1024x1024/Upsample  -         (?, 3, 1024, 1024)   -               \n",
            "G_synthesis/1024x1024/ToRGB     16515     (?, 3, 1024, 1024)   (1, 1, 32, 3)   \n",
            "---                             ---       ---                  ---             \n",
            "Total                           28794126                                       \n",
            "\n",
            "\n",
            "D                     Params    OutputShape          WeightShape     \n",
            "---                   ---       ---                  ---             \n",
            "images_in             -         (?, 3, 1024, 1024)   -               \n",
            "labels_in             -         (?, 0)               -               \n",
            "1024x1024/FromRGB     128       (?, 32, 1024, 1024)  (1, 1, 3, 32)   \n",
            "1024x1024/Conv0       9248      (?, 32, 1024, 1024)  (3, 3, 32, 32)  \n",
            "1024x1024/Conv1_down  18496     (?, 64, 512, 512)    (3, 3, 32, 64)  \n",
            "1024x1024/Skip        2048      (?, 64, 512, 512)    (1, 1, 32, 64)  \n",
            "512x512/Conv0         36928     (?, 64, 512, 512)    (3, 3, 64, 64)  \n",
            "512x512/Conv1_down    73856     (?, 128, 256, 256)   (3, 3, 64, 128) \n",
            "512x512/Skip          8192      (?, 128, 256, 256)   (1, 1, 64, 128) \n",
            "256x256/Conv0         147584    (?, 128, 256, 256)   (3, 3, 128, 128)\n",
            "256x256/Conv1_down    295168    (?, 256, 128, 128)   (3, 3, 128, 256)\n",
            "256x256/Skip          32768     (?, 256, 128, 128)   (1, 1, 128, 256)\n",
            "128x128/Conv0         590080    (?, 256, 128, 128)   (3, 3, 256, 256)\n",
            "128x128/Conv1_down    1180160   (?, 512, 64, 64)     (3, 3, 256, 512)\n",
            "128x128/Skip          131072    (?, 512, 64, 64)     (1, 1, 256, 512)\n",
            "64x64/Conv0           2359808   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "64x64/Conv1_down      2359808   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "64x64/Skip            262144    (?, 512, 32, 32)     (1, 1, 512, 512)\n",
            "32x32/Conv0           2359808   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "32x32/Conv1_down      2359808   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "32x32/Skip            262144    (?, 512, 16, 16)     (1, 1, 512, 512)\n",
            "16x16/Conv0           2359808   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "16x16/Conv1_down      2359808   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "16x16/Skip            262144    (?, 512, 8, 8)       (1, 1, 512, 512)\n",
            "8x8/Conv0             2359808   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "8x8/Conv1_down        2359808   (?, 512, 4, 4)       (3, 3, 512, 512)\n",
            "8x8/Skip              262144    (?, 512, 4, 4)       (1, 1, 512, 512)\n",
            "4x4/MinibatchStddev   -         (?, 513, 4, 4)       -               \n",
            "4x4/Conv              2364416   (?, 512, 4, 4)       (3, 3, 513, 512)\n",
            "4x4/Dense0            4194816   (?, 512)             (8192, 512)     \n",
            "Output                513       (?, 1)               (512, 1)        \n",
            "---                   ---       ---                  ---             \n",
            "Total                 29012513                                       \n",
            "\n",
            "Exporting sample images...\n",
            "2021-10-23 09:52:13.838680: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 88080384 exceeds 10% of system memory.\n",
            "Replicating networks across 1 GPUs...\n",
            "Initializing augmentations...\n",
            "Setting up optimizers...\n",
            "Constructing training graph...\n",
            "Finalizing training ops...\n",
            "2021-10-23 09:53:12.741238: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 16777216 exceeds 10% of system memory.\n",
            "2021-10-23 09:53:12.753564: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 16777216 exceeds 10% of system memory.\n",
            "2021-10-23 09:53:12.807055: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 16777216 exceeds 10% of system memory.\n",
            "2021-10-23 09:53:12.814472: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 16777216 exceeds 10% of system memory.\n",
            "Initializing metrics...\n",
            "Training for 25000 kimg...\n",
            "\n",
            "2021-10-23 09:55:01.000287: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2021-10-23 09:55:08.392291: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2021-10-23 09:55:15.952189: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "tick 0     kimg 0.0      time 4m 59s       sec/tick 119.3   sec/kimg 7456.98 maintenance 179.9  gpumem 10.3  augment 0.000\n",
            "2021-10-23 09:55:48.350694: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2021-10-23 09:55:57.713979: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2021-10-23 09:56:05.163654: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2021-10-23 09:56:12.545890: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2021-10-23 09:56:19.905419: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2021-10-23 09:56:29.302861: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2021-10-23 09:56:36.685429: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIG_uQ9U--P5"
      },
      "source": [
        "### **Generating Images**\n",
        "\n",
        "Nice! When you got your .pkl file that you like best, you can generate images. \n",
        "\n",
        "First, copy the path of your .pkl file, and choose the seeds in which you want generated (ex 100-200 would generate 102 images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51abWxfglcID",
        "outputId": "d38c0cb0-3d45-476c-e4d0-bc72c730fe37"
      },
      "source": [
        "!pip install opensimplex\n",
        "import opensimplex"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opensimplex\n",
            "  Downloading opensimplex-0.3-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opensimplex\n",
            "Successfully installed opensimplex-0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8LsIXMJ--P5"
      },
      "source": [
        "num_images = 10-45\n",
        "network_pkl = \"Latest_pkl_file_in_results_folder\" #@param[]{allow-input: true}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ELlcKfA--P5"
      },
      "source": [
        "Run This to generate the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp4mF6v0--P5",
        "outputId": "215c36e6-5d74-4083-e235-a4d94dd4aa6f"
      },
      "source": [
        "# Trunc min=0.5 max=1.5 \n",
        "# for art higher values, for deepfakes lower values 0.6-0.8\n",
        "trunc = 1.5 #@param {type: \"number\"}\n",
        "network_path = \"Latest_pkl_file_in_results_folder\" #@param[]{allow-input: true}\n",
        "\n",
        "!python generate.py generate-images --outdir=out --trunc={trunc} --seeds=1000-1005 --network={network_path}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00008-dima-art-mirror-11gb-gpu-bg-resumecustom/network-snapshot-000016.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
            "Generating image for seed 1000 (0/6) ...\n",
            "Generating image for seed 1001 (1/6) ...\n",
            "Generating image for seed 1002 (2/6) ...\n",
            "Generating image for seed 1003 (3/6) ...\n",
            "Generating image for seed 1004 (4/6) ...\n",
            "Generating image for seed 1005 (5/6) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def load_networks(path):\n",
        "    stream = open(path, 'rb')\n",
        "    tflib.init_tf()\n",
        "    with stream:\n",
        "        G, D, Gs = pickle.load(stream, encoding='latin1')\n",
        "\n",
        "    return G, D, Gs"
      ],
      "metadata": {
        "id": "5tEZtuEY1Nwc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model of choice\n",
        "import argparse\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import re\n",
        "import sys\n",
        "from io import BytesIO\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "from PIL import Image, ImageDraw\n",
        "import imageio\n",
        "\n",
        "# Choose between these pretrained models - I think 'f' is the best choice:\n",
        "\n",
        "# 1024×1024 faces\n",
        "# stylegan2-ffhq-config-a.pkl\n",
        "# stylegan2-ffhq-config-b.pkl\n",
        "# stylegan2-ffhq-config-c.pkl\n",
        "# stylegan2-ffhq-config-d.pkl\n",
        "# stylegan2-ffhq-config-e.pkl\n",
        "# stylegan2-ffhq-config-f.pkl\n",
        "\n",
        "# 512×384 cars\n",
        "# stylegan2-car-config-a.pkl\n",
        "# stylegan2-car-config-b.pkl\n",
        "# stylegan2-car-config-c.pkl\n",
        "# stylegan2-car-config-d.pkl\n",
        "# stylegan2-car-config-e.pkl\n",
        "# stylegan2-car-config-f.pkl\n",
        "\n",
        "# 256x256 horses\n",
        "# stylegan2-horse-config-a.pkl\n",
        "# stylegan2-horse-config-f.pkl\n",
        "\n",
        "# 256x256 churches\n",
        "# stylegan2-church-config-a.pkl\n",
        "# stylegan2-church-config-f.pkl\n",
        "\n",
        "# 256x256 cats\n",
        "# stylegan2-cat-config-f.pkl\n",
        "# stylegan2-cat-config-a.pkl\n",
        "# network_pkl = \"gdrive:networks/stylegan2-ffhq-config-f.pkl\"\n",
        "\n",
        "# If downloads fails, due to 'Google Drive download quota exceeded' you can try downloading manually from your own Google Drive account\n",
        "#!gdown --id 1UlDmJVLLnBD9SnLSMXeiZRO6g-OMQCA_\n",
        "network_pkl = \"/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00020-squeeze-mirror-11gb-gpu-ada-target0.6-bgc-resumecustom/network-snapshot-000064.pkl\"\n",
        "\n",
        "print('Loading networks from \"%s\"...' % network_pkl)\n",
        "_G, _D, Gs = load_networks(network_pkl)\n",
        "noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]"
      ],
      "metadata": {
        "id": "ws6kfnlY0YS3",
        "outputId": "a0424871-2065-4b82-f2b1-ced0db1133eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00020-squeeze-mirror-11gb-gpu-ada-target0.6-bgc-resumecustom/network-snapshot-000064.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Compiling... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Compiling... Loading... Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful utility functions...\n",
        "\n",
        "# Generates a list of images, based on a list of latent vectors (Z), and a list (or a single constant) of truncation_psi's.\n",
        "def generate_images_in_w_space(dlatents, truncation_psi):\n",
        "    Gs_kwargs = dnnlib.EasyDict()\n",
        "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_kwargs.randomize_noise = False\n",
        "    Gs_kwargs.truncation_psi = truncation_psi\n",
        "    dlatent_avg = Gs.get_var('dlatent_avg') # [component]\n",
        "\n",
        "    imgs = []\n",
        "    for row, dlatent in log_progress(enumerate(dlatents), name = \"Generating images\"):\n",
        "        #row_dlatents = (dlatent[np.newaxis] - dlatent_avg) * np.reshape(truncation_psi, [-1, 1, 1]) + dlatent_avg\n",
        "        dl = (dlatent-dlatent_avg)*truncation_psi   + dlatent_avg\n",
        "        row_images = Gs.components.synthesis.run(dlatent,  **Gs_kwargs)\n",
        "        imgs.append(PIL.Image.fromarray(row_images[0], 'RGB'))\n",
        "    return imgs       \n",
        "\n",
        "def generate_images(zs, truncation_psi):\n",
        "    Gs_kwargs = dnnlib.EasyDict()\n",
        "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_kwargs.randomize_noise = False\n",
        "    if not isinstance(truncation_psi, list):\n",
        "        truncation_psi = [truncation_psi] * len(zs)\n",
        "        \n",
        "    imgs = []\n",
        "    for z_idx, z in log_progress(enumerate(zs), size = len(zs), name = \"Generating images\"):\n",
        "        Gs_kwargs.truncation_psi = truncation_psi[z_idx]\n",
        "        noise_rnd = np.random.RandomState(1) # fix noise\n",
        "        tflib.set_vars({var: noise_rnd.randn(*var.shape.as_list()) for var in noise_vars}) # [height, width]\n",
        "        images = Gs.run(z, None, **Gs_kwargs) # [minibatch, height, width, channel]\n",
        "        imgs.append(PIL.Image.fromarray(images[0], 'RGB'))\n",
        "    return imgs\n",
        "\n",
        "def generate_zs_from_seeds(seeds):\n",
        "    zs = []\n",
        "    for seed_idx, seed in enumerate(seeds):\n",
        "        rnd = np.random.RandomState(seed)\n",
        "        z = rnd.randn(1, *Gs.input_shape[1:]) # [minibatch, component]\n",
        "        zs.append(z)\n",
        "    return zs\n",
        "\n",
        "# Generates a list of images, based on a list of seed for latent vectors (Z), and a list (or a single constant) of truncation_psi's.\n",
        "def generate_images_from_seeds(seeds, truncation_psi):\n",
        "    return generate_images(generate_zs_from_seeds(seeds), truncation_psi)\n",
        "\n",
        "def saveImgs(imgs, location):\n",
        "  for idx, img in log_progress(enumerate(imgs), size = len(imgs), name=\"Saving images\"):\n",
        "    file = location+ str(idx) + \".png\"\n",
        "    img.save(file)\n",
        "\n",
        "def imshow(a, format='png', jpeg_fallback=True):\n",
        "  a = np.asarray(a, dtype=np.uint8)\n",
        "  str_file = BytesIO()\n",
        "  PIL.Image.fromarray(a).save(str_file, format)\n",
        "  im_data = str_file.getvalue()\n",
        "  try:\n",
        "    disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  except IOError:\n",
        "    if jpeg_fallback and format != 'jpeg':\n",
        "      print ('Warning: image was too large to display in format \"{}\"; '\n",
        "             'trying jpeg instead.').format(format)\n",
        "      return imshow(a, format='jpeg')\n",
        "    else:\n",
        "      raise\n",
        "  return disp\n",
        "\n",
        "def showarray(a, fmt='png'):\n",
        "    a = np.uint8(a)\n",
        "    f = StringIO()\n",
        "    PIL.Image.fromarray(a).save(f, fmt)\n",
        "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
        "\n",
        "        \n",
        "def clamp(x, minimum, maximum):\n",
        "    return max(minimum, min(x, maximum))\n",
        "    \n",
        "def drawLatent(image,latents,x,y,x2,y2, color=(255,0,0,100)):\n",
        "  buffer = PIL.Image.new('RGBA', image.size, (0,0,0,0))\n",
        "   \n",
        "  draw = ImageDraw.Draw(buffer)\n",
        "  cy = (y+y2)/2\n",
        "  draw.rectangle([x,y,x2,y2],fill=(255,255,255,180), outline=(0,0,0,180))\n",
        "  for i in range(len(latents)):\n",
        "    mx = x + (x2-x)*(float(i)/len(latents))\n",
        "    h = (y2-y)*latents[i]*0.1\n",
        "    h = clamp(h,cy-y2,y2-cy)\n",
        "    draw.line((mx,cy,mx,cy+h),fill=color)\n",
        "  return PIL.Image.alpha_composite(image,buffer)\n",
        "             \n",
        "  \n",
        "def createImageGrid(images, scale=0.25, rows=1):\n",
        "   w,h = images[0].size\n",
        "   w = int(w*scale)\n",
        "   h = int(h*scale)\n",
        "   height = rows*h\n",
        "   cols = ceil(len(images) / rows)\n",
        "   width = cols*w\n",
        "   canvas = PIL.Image.new('RGBA', (width,height), 'white')\n",
        "   for i,img in enumerate(images):\n",
        "     img = img.resize((w,h), PIL.Image.ANTIALIAS)\n",
        "     canvas.paste(img, (w*(i % cols), h*(i // cols))) \n",
        "   return canvas\n",
        "\n",
        "def convertZtoW(latent, truncation_psi=0.7, truncation_cutoff=9):\n",
        "  dlatent = Gs.components.mapping.run(latent, None) # [seed, layer, component]\n",
        "  dlatent_avg = Gs.get_var('dlatent_avg') # [component]\n",
        "  for i in range(truncation_cutoff):\n",
        "    dlatent[0][i] = (dlatent[0][i]-dlatent_avg)*truncation_psi + dlatent_avg\n",
        "    \n",
        "  return dlatent\n",
        "\n",
        "def interpolate(zs, steps):\n",
        "   out = []\n",
        "   for i in range(len(zs)-1):\n",
        "    for index in range(steps):\n",
        "     fraction = index/float(steps) \n",
        "     out.append(zs[i+1]*fraction + zs[i]*(1-fraction))\n",
        "   return out\n",
        "\n",
        "# Taken from https://github.com/alexanderkuk/log-progress\n",
        "def log_progress(sequence, every=1, size=None, name='Items'):\n",
        "    from ipywidgets import IntProgress, HTML, VBox\n",
        "    from IPython.display import display\n",
        "\n",
        "    is_iterator = False\n",
        "    if size is None:\n",
        "        try:\n",
        "            size = len(sequence)\n",
        "        except TypeError:\n",
        "            is_iterator = True\n",
        "    if size is not None:\n",
        "        if every is None:\n",
        "            if size <= 200:\n",
        "                every = 1\n",
        "            else:\n",
        "                every = int(size / 200)     # every 0.5%\n",
        "    else:\n",
        "        assert every is not None, 'sequence is iterator, set every'\n",
        "\n",
        "    if is_iterator:\n",
        "        progress = IntProgress(min=0, max=1, value=1)\n",
        "        progress.bar_style = 'info'\n",
        "    else:\n",
        "        progress = IntProgress(min=0, max=size, value=0)\n",
        "    label = HTML()\n",
        "    box = VBox(children=[label, progress])\n",
        "    display(box)\n",
        "\n",
        "    index = 0\n",
        "    try:\n",
        "        for index, record in enumerate(sequence, 1):\n",
        "            if index == 1 or index % every == 0:\n",
        "                if is_iterator:\n",
        "                    label.value = '{name}: {index} / ?'.format(\n",
        "                        name=name,\n",
        "                        index=index\n",
        "                    )\n",
        "                else:\n",
        "                    progress.value = index\n",
        "                    label.value = u'{name}: {index} / {size}'.format(\n",
        "                        name=name,\n",
        "                        index=index,\n",
        "                        size=size\n",
        "                    )\n",
        "            yield record\n",
        "    except:\n",
        "        progress.bar_style = 'danger'\n",
        "        raise\n",
        "    else:\n",
        "        progress.bar_style = 'success'\n",
        "        progress.value = index\n",
        "        label.value = \"{name}: {index}\".format(\n",
        "            name=name,\n",
        "            index=str(index or '?')\n",
        "        )"
      ],
      "metadata": {
        "id": "vWw3ESqF0fDf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate.py generate-images --outdir=out6 --trunc=0.7 --seeds=1781 --save_vector --network={network_pkl}"
      ],
      "metadata": {
        "id": "tbKd_bTp1vh-",
        "outputId": "6adf58bb-0f05-4904-a0a7-1325aada86c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00020-squeeze-mirror-11gb-gpu-ada-target0.6-bgc-resumecustom/network-snapshot-000064.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
            "Generating image for seed 1781 (0/1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "arr = np.load('/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/out6/vectors/seed1781.npy')\n",
        "arr1 = convertZtoW(arr)\n",
        "np.save('/content/seed1781.npy', arr1)"
      ],
      "metadata": {
        "id": "vV45JQR42rGf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent1 = np.load('/content/drive/MyDrive/final_interpolation/seed1758.npy')\n",
        "latent2 = np.load('/content/drive/MyDrive/final_interpolation/seed1168.npy')\n",
        "#latent3 = np.load('/content/stylegan2/projection/latent-bruce.npy')\n",
        "#latent4 = np.load('/content/stylegan2/projection/latent-britney.npy')\n",
        "#noise_vars = np.load('/content/stylegan2/projection/noise.npy',allow_pickle=True)\n",
        "\n",
        "#imgs = generate_images_in_w_space([latent2],0.7)\n",
        "imgs = generate_images_in_w_space(interpolate([latent1,latent2],300),0.7)\n",
        "\n",
        "#%mkdir interpolations\n",
        "%mkdir /content/drive/MyDrive/final_interpolation/300_final_sq5-6\n",
        "saveImgs(imgs,'/content/drive/MyDrive/final_interpolation/300_final_sq5-6/')  "
      ],
      "metadata": {
        "id": "2LTQtor00i0R",
        "outputId": "0f081202-cd54-413b-e16c-fdc3fe87ee95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "cac38f8d43a84ec29dd2076ad7d8da09",
            "cb8a60b3a51f4883adca5135e8ead948",
            "1b6f2e85b1334a67949f9fba03c7fb3d",
            "edb08bda75a243deae18989b518ac6c3",
            "f3d7f751f9d448d0bbaad17183b99525",
            "b6f036586ea04c0e97b36337f4dfd23c",
            "03f87079d67a42698d4e21c0a140e1dc",
            "8750e9f2bce1419298b83db8e02f1919",
            "56fc021f48ad4b2d9b0ad37a55b557b1",
            "b79191a9c7a24791b94d84b53562fcd9",
            "ee291997d8f94a7bad1c5273c0c94b62",
            "d73267fc01ef4b5eac3ae430d821ed06",
            "cb4975a2be1949118de11027212d4d69",
            "4458838a06e6460ba04c95764d4f4dd6",
            "d3139d7b7ef74f80b463ecd1606bc9ac",
            "5a8683d536cd4a06b43f72f842623708"
          ]
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cac38f8d43a84ec29dd2076ad7d8da09",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(HTML(value=''), IntProgress(value=1, bar_style='info', max=1)))"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56fc021f48ad4b2d9b0ad37a55b557b1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(HTML(value=''), IntProgress(value=0, max=300)))"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS4QMMuN0Y1D"
      },
      "source": [
        "## Latent Walk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAJqFOPf--P5"
      },
      "source": [
        "You can now zip these images to easily download into your local machine. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ee_hA2d--P5"
      },
      "source": [
        "!zip -r generated-images.zip /out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgGMGrZx--P6"
      },
      "source": [
        "**This is not the end of the possibilities. With your pkl file, there are countless applications that I could not cover in one document. This is an amazing tool to not only have fun but dip your toes into the AI world.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hdt0S2XyxFo"
      },
      "source": [
        "## **This is not the end of the possibilities. With your pkl file, there are countless applications that I could not cover in one document. This is an amazing tool to not only have fun but dip your toes into the AI world.**"
      ]
    }
  ]
}